{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¬ Avenue Anomaly Detection - Vectorized High Accuracy Version\n",
        "\n",
        "## I3D + Simple Autoencoder + Feature Normalization + Corruption Handling + Full Vectorization\n",
        "\n",
        "This notebook implements a **video anomaly detection** pipeline using:\n",
        "\n",
        "1. **I3D (Inflated 3D ConvNet)** - Pre-trained video feature extractor from Facebook's PyTorchVideo\n",
        "2. **Simple Autoencoder** - Learns to reconstruct normal video features\n",
        "3. **Corruption Handler** - Automatic detection and correction of noisy/inverted test images\n",
        "4. **Anomaly Scoring** - Uses reconstruction error to identify anomalies\n",
        "5. **Visualizations** - Includes pixel-level heatmaps (Grad-CAM) and t-SNE embeddings\n",
        "\n",
        "### Key Features:\n",
        "- âœ… Vectorized operations for speed\n",
        "- âœ… Feature caching for faster re-runs\n",
        "- âœ… Proper train/test normalization\n",
        "- âœ… **Automatic corruption fix for test data** (noise removal & inversion correction)\n",
        "- âœ… Pixel-level anomaly heatmaps (Grad-CAM)\n",
        "- âœ… t-SNE visualization of feature space\n",
        "- âœ… AUC computation with ground truth\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ðŸ“¦ Import Libraries\n",
        "\n",
        "Import all necessary libraries for data processing, deep learning, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# PyTorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. âš™ï¸ Configuration\n",
        "\n",
        "Set up all hyperparameters, paths, and visualization toggles.\n",
        "\n",
        "### Configuration Parameters:\n",
        "- **IMG_SIZE**: Input image size (224x224 for I3D)\n",
        "- **SEQ_LEN**: Number of frames per video clip (16 for I3D)\n",
        "- **BATCH_SIZE**: Batch size for I3D feature extraction\n",
        "- **TRAIN_BATCH**: Batch size for autoencoder training\n",
        "- **NUM_EPOCHS**: Number of training epochs\n",
        "- **ENABLE_HEATMAPS**: Toggle for pixel-level heatmap visualization\n",
        "- **ENABLE_TSNE**: Toggle for t-SNE visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL & TRAINING HYPERPARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "IMG_SIZE = 224              # Input image size (I3D requirement)\n",
        "SEQ_LEN = 16                # Number of frames per clip (I3D requirement)\n",
        "BATCH_SIZE = 4              # Batch size for I3D feature extraction\n",
        "TRAIN_BATCH = 128           # Batch size for autoencoder training\n",
        "NUM_EPOCHS = 100            # Number of training epochs\n",
        "LEARNING_RATE = 5e-4        # Learning rate for AdamW optimizer\n",
        "WEIGHT_DECAY = 1e-5         # Weight decay for regularization\n",
        "NUM_WORKERS = 4             # Number of parallel workers for data loading\n",
        "\n",
        "# =============================================================================\n",
        "# CACHE DIRECTORIES\n",
        "# =============================================================================\n",
        "\n",
        "CACHE_DIR = \"/kaggle/working/cache\"\n",
        "FEATURE_CACHE = os.path.join(CACHE_DIR, \"features\")\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALIZATION TOGGLES\n",
        "# =============================================================================\n",
        "\n",
        "# Heatmap Visualization (Grad-CAM)\n",
        "ENABLE_HEATMAPS = True        # Set to False to disable heatmap generation\n",
        "HEATMAP_FRAMES = 8            # Number of frames to visualize\n",
        "HEATMAP_VIDEO_IDX = 1         # Which test video to visualize (1-indexed)\n",
        "\n",
        "# t-SNE Visualization\n",
        "ENABLE_TSNE = True            # Set to False to disable t-SNE visualization\n",
        "TSNE_PERPLEXITY = 30          # t-SNE perplexity parameter (typical: 5-50)\n",
        "TSNE_MAX_SAMPLES = 2000       # Max samples for t-SNE (for speed)\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET PATHS\n",
        "# =============================================================================\n",
        "\n",
        "DATA_ROOT = \"/kaggle/input/avenue-dataset/Avenue_Corrupted/Dataset\"\n",
        "TRAIN_DIR = os.path.join(DATA_ROOT, \"training_videos\")\n",
        "TEST_DIR = os.path.join(DATA_ROOT, \"testing_videos\")\n",
        "OUTPUT_CSV = \"avenue_scores.csv\"\n",
        "\n",
        "print(\"Configuration loaded!\")\n",
        "print(f\"  - Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"  - Sequence length: {SEQ_LEN} frames\")\n",
        "print(f\"  - Training epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  - Heatmaps enabled: {ENABLE_HEATMAPS}\")\n",
        "print(f\"  - t-SNE enabled: {ENABLE_TSNE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# REPRODUCIBILITY & DEVICE SETUP\n",
        "# =============================================================================\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Enable cuDNN benchmark for faster training on fixed input sizes\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Select device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create cache directory for storing features\n",
        "os.makedirs(FEATURE_CACHE, exist_ok=True)\n",
        "print(f\"Feature cache directory: {FEATURE_CACHE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ðŸ–¼ï¸ Vectorized Image Loading & Corruption Handler\n",
        "\n",
        "Efficient image loading utilities using:\n",
        "- **ThreadPoolExecutor** for parallel I/O operations\n",
        "- **Vectorized normalization** using numpy broadcasting\n",
        "- **ImageNet-style normalization** (mean=0.45, std=0.225)\n",
        "\n",
        "### ðŸ”§ Corruption Handler (Test Data Only)\n",
        "\n",
        "\n",
        "\n",
        "The `CorruptionHandlerFast` class automatically detects and fixes common corruptions in test images:\n",
        "\n",
        "| Corruption Type | Detection Method | Fix Applied |\n",
        "|-----------------|------------------|-------------|\n",
        "| **Noisy Images** | Laplacian variance > threshold (800) | Gaussian blur (5x5 kernel) |\n",
        "| **Inverted Images** | Lower brightness > Upper brightness + threshold (25) | Vertical flip |\n",
        "\n",
        "**Key Features:**\n",
        "- ðŸš€ **Fast**: Uses downsampled images (64x64) for noise detection\n",
        "- ðŸ”’ **Thread-safe**: Statistics tracking with mutex locks for parallel processing\n",
        "- ðŸ“Š **Statistics**: Tracks total processed, inverted fixed, and noisy fixed counts\n",
        "\n",
        "**Note**: Corruption handling is applied **only during test feature extraction** to preserve training data integrity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# NORMALIZATION CONSTANTS\n",
        "# =============================================================================\n",
        "\n",
        "# Pre-shaped for broadcasting: (1, 1, 1, 3) to match (N, H, W, C)\n",
        "MEAN = np.array([0.45, 0.45, 0.45], dtype=np.float32).reshape(1, 1, 1, 3)\n",
        "STD = np.array([0.225, 0.225, 0.225], dtype=np.float32).reshape(1, 1, 1, 3)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CORRUPTION HANDLER (TEST ONLY) - FAST VECTORIZED\n",
        "# =============================================================================\n",
        "\n",
        "class CorruptionHandlerFast:\n",
        "    \"\"\"\n",
        "    FAST corruption handler for inverted/noisy images.\n",
        "    Optimized for speed with vectorized operations.\n",
        "    \n",
        "    Attributes:\n",
        "        noise_threshold (int): Laplacian variance threshold for noise detection (default: 800)\n",
        "        inversion_threshold (int): Brightness difference threshold for inversion detection (default: 25)\n",
        "        stats (dict): Tracks counts of processed, inverted, and noisy images\n",
        "        \n",
        "    Usage:\n",
        "        handler = CorruptionHandlerFast()\n",
        "        corrected_img = handler.fix_image_fast(img)\n",
        "        stats = handler.get_stats()\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, noise_threshold=800, inversion_threshold=25):\n",
        "        \"\"\"\n",
        "        Initialize the corruption handler with detection thresholds.\n",
        "        \n",
        "        Args:\n",
        "            noise_threshold: Laplacian variance above this indicates noisy image\n",
        "            inversion_threshold: Brightness diff above this indicates inverted image\n",
        "        \"\"\"\n",
        "        self.noise_threshold = noise_threshold\n",
        "        self.inversion_threshold = inversion_threshold\n",
        "        self.stats = {'inverted': 0, 'noisy': 0, 'total': 0}\n",
        "        self._lock = None  # For thread-safe stats\n",
        "    \n",
        "    def _get_lock(self):\n",
        "        \"\"\"Lazy initialization of threading lock for thread-safe stats updates.\"\"\"\n",
        "        if self._lock is None:\n",
        "            import threading\n",
        "            self._lock = threading.Lock()\n",
        "        return self._lock\n",
        "    \n",
        "    def fix_image_fast(self, img):\n",
        "        \"\"\"\n",
        "        FAST corruption fix - minimal operations for speed.\n",
        "        \n",
        "        Detection Methods:\n",
        "            - Noise: Laplacian variance on downsampled (64x64) grayscale image\n",
        "            - Inversion: Compare mean brightness of upper vs lower third of image\n",
        "        \n",
        "        Correction Methods:\n",
        "            - Noise: Gaussian blur (5x5) - 10x faster than bilateral filter\n",
        "            - Inversion: Vertical flip (cv2.flip with axis=0)\n",
        "        \n",
        "        Args:\n",
        "            img: Input RGB image (H, W, 3) as numpy array\n",
        "            \n",
        "        Returns:\n",
        "            corrected: Corrected RGB image (H, W, 3)\n",
        "        \"\"\"\n",
        "        # Convert to grayscale once for all detection checks\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        h = gray.shape[0]\n",
        "        \n",
        "        # ----- NOISE DETECTION -----\n",
        "        # Downsample to 64x64 for faster Laplacian computation\n",
        "        # High variance in Laplacian indicates edges/noise\n",
        "        gray_small = cv2.resize(gray, (64, 64), interpolation=cv2.INTER_AREA)\n",
        "        noise_level = cv2.Laplacian(gray_small, cv2.CV_64F).var()\n",
        "        is_noisy = noise_level > self.noise_threshold\n",
        "        \n",
        "        # ----- INVERSION DETECTION -----\n",
        "        # Compare brightness of upper third vs lower third\n",
        "        # In normal outdoor scenes, sky (upper) is typically brighter than ground (lower)\n",
        "        # If lower is significantly brighter, image may be vertically inverted\n",
        "        upper_brightness = gray[:h//3].mean()\n",
        "        lower_brightness = gray[2*h//3:].mean()\n",
        "        is_inverted = (lower_brightness - upper_brightness) > self.inversion_threshold\n",
        "        \n",
        "        # ----- APPLY CORRECTIONS -----\n",
        "        corrected = img\n",
        "        \n",
        "        if is_noisy:\n",
        "            # Gaussian blur for fast denoising (bilateral is more accurate but 10x slower)\n",
        "            corrected = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "        \n",
        "        if is_inverted:\n",
        "            # Vertical flip to correct inverted images\n",
        "            corrected = cv2.flip(corrected, 0)\n",
        "        \n",
        "        # ----- UPDATE STATISTICS (Thread-safe) -----\n",
        "        with self._get_lock():\n",
        "            self.stats['total'] += 1\n",
        "            if is_noisy:\n",
        "                self.stats['noisy'] += 1\n",
        "            if is_inverted:\n",
        "                self.stats['inverted'] += 1\n",
        "        \n",
        "        return corrected\n",
        "    \n",
        "    def get_stats(self):\n",
        "        \"\"\"\n",
        "        Get corruption correction statistics.\n",
        "        \n",
        "        Returns:\n",
        "            dict: Contains total_processed, inverted_fixed, noisy_fixed,\n",
        "                  inverted_pct, and noisy_pct\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'total_processed': self.stats['total'],\n",
        "            'inverted_fixed': self.stats['inverted'],\n",
        "            'noisy_fixed': self.stats['noisy'],\n",
        "            'inverted_pct': 100 * self.stats['inverted'] / max(1, self.stats['total']),\n",
        "            'noisy_pct': 100 * self.stats['noisy'] / max(1, self.stats['total'])\n",
        "        }\n",
        "    \n",
        "    def reset_stats(self):\n",
        "        \"\"\"Reset all statistics counters to zero.\"\"\"\n",
        "        self.stats = {'inverted': 0, 'noisy': 0, 'total': 0}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# GLOBAL CORRUPTION HANDLER (Singleton Pattern)\n",
        "# =============================================================================\n",
        "# Single instance ensures consistent statistics across all test image loading\n",
        "\n",
        "_test_corruption_handler = None\n",
        "\n",
        "def get_test_corruption_handler():\n",
        "    \"\"\"\n",
        "    Get the global corruption handler instance (singleton pattern).\n",
        "    Creates the handler on first call, returns existing instance thereafter.\n",
        "    \n",
        "    Returns:\n",
        "        CorruptionHandlerFast: Global corruption handler instance\n",
        "    \"\"\"\n",
        "    global _test_corruption_handler\n",
        "    if _test_corruption_handler is None:\n",
        "        _test_corruption_handler = CorruptionHandlerFast()\n",
        "    return _test_corruption_handler\n",
        "\n",
        "\n",
        "def load_single_image_raw(path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load a single image as uint8 (no normalization yet).\n",
        "    \n",
        "    Args:\n",
        "        path: Path to the image file\n",
        "        \n",
        "    Returns:\n",
        "        img: (H, W, C) uint8 numpy array\n",
        "    \"\"\"\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        # Return black image if loading fails\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
        "    # Convert BGR to RGB and resize\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_images_vectorized(paths: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    VECTORIZED: Load multiple images in parallel with efficient normalization.\n",
        "    \n",
        "    Uses ThreadPoolExecutor for parallel I/O and vectorized numpy operations\n",
        "    for normalization.\n",
        "    \n",
        "    Args:\n",
        "        paths: List of image file paths\n",
        "        \n",
        "    Returns:\n",
        "        batch: (N, H, W, C) float32 normalized numpy array\n",
        "    \"\"\"\n",
        "    # Parallel I/O loading using thread pool\n",
        "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
        "        images = list(executor.map(load_single_image_raw, paths))\n",
        "    \n",
        "    # VECTORIZED: Stack and normalize in one operation\n",
        "    batch = np.stack(images, axis=0).astype(np.float32)  # (N, H, W, C)\n",
        "    batch = (batch / 255.0 - MEAN) / STD  # Vectorized normalization\n",
        "    \n",
        "    return batch\n",
        "\n",
        "\n",
        "def load_clip_vectorized(frame_paths: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load a video clip with vectorized normalization.\n",
        "    \n",
        "    Args:\n",
        "        frame_paths: List of frame image paths\n",
        "        \n",
        "    Returns:\n",
        "        clip: (T, H, W, C) float32 normalized numpy array\n",
        "    \"\"\"\n",
        "    return load_images_vectorized(frame_paths)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TEST-SPECIFIC LOADING (WITH FAST CORRUPTION FIX)\n",
        "# =============================================================================\n",
        "# These functions are used ONLY for test data loading.\n",
        "# They apply corruption correction (noise removal + inversion fix) to each frame.\n",
        "# Training data uses the standard load_clip_vectorized() without corruption handling.\n",
        "# =============================================================================\n",
        "\n",
        "def _load_and_fix_single(path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load and fix a single image with corruption correction.\n",
        "    Designed for parallel execution via ThreadPoolExecutor.\n",
        "    \n",
        "    Pipeline:\n",
        "        1. Load image from disk (BGR format)\n",
        "        2. Convert BGR -> RGB\n",
        "        3. Apply corruption fix (noise removal + inversion correction)\n",
        "        4. Resize to IMG_SIZE x IMG_SIZE\n",
        "    \n",
        "    Args:\n",
        "        path: Path to the image file\n",
        "        \n",
        "    Returns:\n",
        "        img: Corrected and resized RGB image (H, W, 3) as uint8\n",
        "    \"\"\"\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        # Return black image if loading fails\n",
        "        return np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
        "    \n",
        "    # Convert BGR to RGB (OpenCV loads as BGR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Apply corruption correction (noise removal + inversion fix)\n",
        "    img = get_test_corruption_handler().fix_image_fast(img)\n",
        "    \n",
        "    # Resize to target size\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_clip_vectorized_test(frame_paths: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    FAST VECTORIZED: Load TEST video clip with corruption correction.\n",
        "    \n",
        "    This function is specifically for TEST data loading. It applies the\n",
        "    corruption handler to each frame to fix noisy/inverted images before\n",
        "    feature extraction.\n",
        "    \n",
        "    Pipeline:\n",
        "        1. Parallel load + fix all frames using ThreadPoolExecutor\n",
        "        2. Stack frames into batch array\n",
        "        3. Apply ImageNet-style normalization\n",
        "    \n",
        "    Args:\n",
        "        frame_paths: List of paths to frame images (length = SEQ_LEN)\n",
        "        \n",
        "    Returns:\n",
        "        clip: Normalized clip tensor (T, H, W, C) as float32\n",
        "    \"\"\"\n",
        "    # Parallel load + fix all frames at once for speed\n",
        "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
        "        images = list(executor.map(_load_and_fix_single, frame_paths))\n",
        "    \n",
        "    # VECTORIZED: Stack and normalize\n",
        "    batch = np.stack(images, axis=0).astype(np.float32)\n",
        "    batch = (batch / 255.0 - MEAN) / STD\n",
        "    return batch\n",
        "\n",
        "\n",
        "def get_frame_num(path: str) -> int:\n",
        "    \"\"\"\n",
        "    Extract frame number from filename.\n",
        "    \n",
        "    Args:\n",
        "        path: Path to frame image\n",
        "        \n",
        "    Returns:\n",
        "        frame_num: Integer frame number extracted from filename\n",
        "    \"\"\"\n",
        "    name = os.path.basename(path)\n",
        "    digits = ''.join(filter(str.isdigit, os.path.splitext(name)[0]))\n",
        "    return int(digits) if digits else 0\n",
        "\n",
        "\n",
        "print(\"Image loading functions defined!\")\n",
        "print(\"Corruption handler for test images enabled!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. I3D Feature Extractor\n",
        "\n",
        "I3D (Inflated 3D ConvNet) is a powerful video understanding model that:\n",
        "- Uses 3D convolutions to capture spatiotemporal patterns\n",
        "- Pre-trained on Kinetics dataset for action recognition\n",
        "- Outputs 2048-dimensional feature vectors per video clip\n",
        "\n",
        "We use a Singleton pattern to ensure the model is loaded only once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class I3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Singleton I3D model wrapper.\n",
        "    \n",
        "    Uses Facebook's PyTorchVideo i3d_r50 (ResNet-50 backbone).\n",
        "    The model is loaded only once and reused across all feature extractions.\n",
        "    \n",
        "    Architecture:\n",
        "        - Input: (B, C, T, H, W) = (batch, 3, 16, 224, 224)\n",
        "        - Output: (B, 2048) feature vectors\n",
        "    \"\"\"\n",
        "    _instance = None\n",
        "    \n",
        "    def __new__(cls):\n",
        "        # Singleton pattern: return existing instance if available\n",
        "        if cls._instance is None:\n",
        "            cls._instance = super().__new__(cls)\n",
        "            cls._instance._initialized = False\n",
        "        return cls._instance\n",
        "    \n",
        "    def __init__(self):\n",
        "        if self._initialized:\n",
        "            return\n",
        "        super().__init__()\n",
        "        \n",
        "        print(\"Loading I3D model from PyTorchVideo...\")\n",
        "        \n",
        "        # Load pretrained I3D ResNet-50\n",
        "        self.model = torch.hub.load(\n",
        "            'facebookresearch/pytorchvideo', \n",
        "            'i3d_r50', \n",
        "            pretrained=True\n",
        "        )\n",
        "        \n",
        "        # Remove the classification head, keep features only\n",
        "        # Replace projection with identity to get raw 2048-dim features\n",
        "        self.model.blocks[-1].proj = nn.Identity()\n",
        "        \n",
        "        # Freeze all parameters (we only use it for feature extraction)\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        self.eval()\n",
        "        self._initialized = True\n",
        "        print(\"I3D model loaded successfully!\")\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Extract features from video clips.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (B, C, T, H, W)\n",
        "            \n",
        "        Returns:\n",
        "            features: Output tensor of shape (B, 2048)\n",
        "        \"\"\"\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Extraction Functions\n",
        "\n",
        "These functions handle the extraction of I3D features from video frames:\n",
        "\n",
        "1. extract_features_vectorized: Extract and normalize features from training videos\n",
        "2. extract_test_features_vectorized: Extract test features using training statistics\n",
        "\n",
        "Features are cached to disk for faster subsequent runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features_vectorized(data_dir: str, name: str, stride: int = 1) -> Tuple:\n",
        "    \"\"\"\n",
        "    VECTORIZED feature extraction with caching and proper normalization.\n",
        "    \n",
        "    This function:\n",
        "    1. Creates video clips with specified stride\n",
        "    2. Extracts I3D features for each clip\n",
        "    3. Computes normalization statistics (mean, std)\n",
        "    4. Normalizes all features\n",
        "    5. Caches results to disk\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory containing video folders\n",
        "        name: Name for caching (e.g., \"train\")\n",
        "        stride: Stride between consecutive clips\n",
        "        \n",
        "    Returns:\n",
        "        features: (N, 2048) normalized tensor\n",
        "        clips: List of clip metadata\n",
        "        videos: Dict of video metadata\n",
        "        stats: Normalization statistics (mean, std)\n",
        "    \"\"\"\n",
        "    cache_file = os.path.join(FEATURE_CACHE, f\"{name}_v2_s{stride}.pt\")\n",
        "    \n",
        "    # Check cache first\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"[CACHE HIT] Loading {name}\")\n",
        "        data = torch.load(cache_file, weights_only=False)\n",
        "        return data['features'], data['clips'], data['videos'], data['stats']\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Extracting features: {name} (stride={stride})\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Load I3D model\n",
        "    i3d = I3D().to(device)\n",
        "    \n",
        "    # Collect video info\n",
        "    videos = {}\n",
        "    clips = []\n",
        "    \n",
        "    video_dirs = sorted([d for d in os.listdir(data_dir) \n",
        "                        if os.path.isdir(os.path.join(data_dir, d))])\n",
        "    \n",
        "    print(f\"Found {len(video_dirs)} videos\")\n",
        "    \n",
        "    # Process each video\n",
        "    for vid_id, vid_name in enumerate(video_dirs, 1):\n",
        "        vid_path = os.path.join(data_dir, vid_name)\n",
        "        frames = sorted(glob.glob(os.path.join(vid_path, \"*.jpg\")))\n",
        "        \n",
        "        if len(frames) < SEQ_LEN:\n",
        "            continue\n",
        "        \n",
        "        # VECTORIZED: Extract all frame numbers at once\n",
        "        frame_nums = np.array([get_frame_num(f) for f in frames])\n",
        "        \n",
        "        videos[vid_id] = {\n",
        "            'name': vid_name,\n",
        "            'frames': frames,\n",
        "            'nums': frame_nums,\n",
        "            'n': len(frames)\n",
        "        }\n",
        "        \n",
        "        # Create clips with specified stride\n",
        "        n_clips_vid = (len(frames) - SEQ_LEN) // stride + 1\n",
        "        for i in range(n_clips_vid):\n",
        "            start = i * stride\n",
        "            clips.append({\n",
        "                'vid': vid_id,\n",
        "                'start': start,\n",
        "                'paths': frames[start:start + SEQ_LEN],\n",
        "                'nums': frame_nums[start:start + SEQ_LEN]\n",
        "            })\n",
        "    \n",
        "    n_clips = len(clips)\n",
        "    print(f\"Total clips: {n_clips}\")\n",
        "    \n",
        "    # VECTORIZED: Pre-allocate feature tensor\n",
        "    features = torch.zeros(n_clips, 2048, dtype=torch.float32)\n",
        "    \n",
        "    # Extract features in batches\n",
        "    for i in tqdm(range(0, n_clips, BATCH_SIZE), desc=\"Extracting I3D\"):\n",
        "        batch_clips = clips[i:i + BATCH_SIZE]\n",
        "        batch_size = len(batch_clips)\n",
        "        \n",
        "        # VECTORIZED: Load all clips for this batch\n",
        "        batch_data = []\n",
        "        for clip in batch_clips:\n",
        "            clip_frames = load_clip_vectorized(clip['paths'])\n",
        "            batch_data.append(clip_frames)\n",
        "        \n",
        "        # Stack: (B, T, H, W, C)\n",
        "        batch_array = np.stack(batch_data, axis=0)\n",
        "        \n",
        "        # VECTORIZED: Convert to PyTorch and permute\n",
        "        # (B, T, H, W, C) -> (B, C, T, H, W)\n",
        "        batch_tensor = torch.from_numpy(batch_array).permute(0, 4, 1, 2, 3)\n",
        "        batch_tensor = batch_tensor.to(device)\n",
        "        \n",
        "        # Extract with mixed precision for speed\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            feats = i3d(batch_tensor)\n",
        "        \n",
        "        # Store in pre-allocated tensor\n",
        "        features[i:i + batch_size] = feats.cpu()\n",
        "    \n",
        "    # VECTORIZED: Compute normalization statistics\n",
        "    stats = {\n",
        "        'mean': features.mean(dim=0),      # (2048,)\n",
        "        'std': features.std(dim=0) + 1e-8  # (2048,) with epsilon for stability\n",
        "    }\n",
        "    \n",
        "    # VECTORIZED: Normalize all features at once\n",
        "    features = (features - stats['mean'].unsqueeze(0)) / stats['std'].unsqueeze(0)\n",
        "    \n",
        "    print(f\"Feature stats after normalization: mean={features.mean():.4f}, std={features.std():.4f}\")\n",
        "    \n",
        "    # Save cache\n",
        "    torch.save({\n",
        "        'features': features,\n",
        "        'clips': clips,\n",
        "        'videos': videos,\n",
        "        'stats': stats\n",
        "    }, cache_file)\n",
        "    \n",
        "    # Clean up GPU memory\n",
        "    del i3d\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return features, clips, videos, stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_test_features_vectorized(data_dir: str, train_stats: dict, stride: int = 1) -> Tuple:\n",
        "    \"\"\"\n",
        "    VECTORIZED test feature extraction using TRAINING normalization stats.\n",
        "    Includes corruption correction for inverted/noisy images.\n",
        "    \n",
        "    Important: Uses training mean/std for normalization to ensure\n",
        "    consistent feature scaling between train and test.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory containing test video folders\n",
        "        train_stats: Normalization statistics from training data\n",
        "        stride: Stride between consecutive clips\n",
        "        \n",
        "    Returns:\n",
        "        features: (N, 2048) normalized tensor\n",
        "        clips: List of clip metadata\n",
        "        videos: Dict of video metadata\n",
        "    \"\"\"\n",
        "    # Reset corruption handler stats for fresh count\n",
        "    get_test_corruption_handler().reset_stats()\n",
        "    \n",
        "    cache_file = os.path.join(FEATURE_CACHE, f\"test_v2_s{stride}_corrected.pt\")\n",
        "    \n",
        "    # Check cache first\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"[CACHE HIT] Loading test features\")\n",
        "        data = torch.load(cache_file, weights_only=False)\n",
        "        return data['features'], data['clips'], data['videos']\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Extracting test features (stride={stride})\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Load I3D model\n",
        "    i3d = I3D().to(device)\n",
        "    \n",
        "    videos = {}\n",
        "    clips = []\n",
        "    \n",
        "    video_dirs = sorted([d for d in os.listdir(data_dir) \n",
        "                        if os.path.isdir(os.path.join(data_dir, d))])\n",
        "    \n",
        "    print(f\"Found {len(video_dirs)} videos\")\n",
        "    \n",
        "    # Process each video\n",
        "    for vid_id, vid_name in enumerate(video_dirs, 1):\n",
        "        vid_path = os.path.join(data_dir, vid_name)\n",
        "        frames = sorted(glob.glob(os.path.join(vid_path, \"*.jpg\")))\n",
        "        \n",
        "        if len(frames) < SEQ_LEN:\n",
        "            continue\n",
        "        \n",
        "        frame_nums = np.array([get_frame_num(f) for f in frames])\n",
        "        \n",
        "        videos[vid_id] = {\n",
        "            'name': vid_name,\n",
        "            'frames': frames,\n",
        "            'nums': frame_nums,\n",
        "            'n': len(frames)\n",
        "        }\n",
        "        \n",
        "        # Create clips\n",
        "        n_clips_vid = (len(frames) - SEQ_LEN) // stride + 1\n",
        "        for i in range(n_clips_vid):\n",
        "            start = i * stride\n",
        "            clips.append({\n",
        "                'vid': vid_id,\n",
        "                'start': start,\n",
        "                'paths': frames[start:start + SEQ_LEN],\n",
        "                'nums': frame_nums[start:start + SEQ_LEN]\n",
        "            })\n",
        "    \n",
        "    n_clips = len(clips)\n",
        "    print(f\"Total clips: {n_clips}\")\n",
        "    \n",
        "    # Pre-allocate feature tensor\n",
        "    features = torch.zeros(n_clips, 2048, dtype=torch.float32)\n",
        "    \n",
        "    # Use larger batch for faster extraction\n",
        "    TEST_BATCH = BATCH_SIZE * 2  # Double batch size for test\n",
        "    \n",
        "    # Extract features in batches with corruption correction\n",
        "    for i in tqdm(range(0, n_clips, TEST_BATCH), desc=\"Extracting I3D (fast)\"):\n",
        "        batch_clips = clips[i:i + TEST_BATCH]\n",
        "        batch_size = len(batch_clips)\n",
        "        \n",
        "        # PARALLEL: Load all clips for this batch concurrently with corruption fix\n",
        "        with ThreadPoolExecutor(max_workers=min(batch_size, 8)) as executor:\n",
        "            batch_data = list(executor.map(\n",
        "                lambda c: load_clip_vectorized_test(c['paths']), \n",
        "                batch_clips\n",
        "            ))\n",
        "        \n",
        "        batch_array = np.stack(batch_data, axis=0)\n",
        "        batch_tensor = torch.from_numpy(batch_array).permute(0, 4, 1, 2, 3).to(device)\n",
        "        \n",
        "        with torch.amp.autocast('cuda'):\n",
        "            feats = i3d(batch_tensor)\n",
        "        \n",
        "        features[i:i + batch_size] = feats.cpu()\n",
        "    \n",
        "    # Print corruption correction statistics\n",
        "    handler_stats = get_test_corruption_handler().get_stats()\n",
        "    print(f\"\\n[CORRUPTION CORRECTION STATS]\")\n",
        "    print(f\"  Total frames processed: {handler_stats['total_processed']}\")\n",
        "    print(f\"  Inverted images fixed: {handler_stats['inverted_fixed']} ({handler_stats['inverted_pct']:.1f}%)\")\n",
        "    print(f\"  Noisy images fixed: {handler_stats['noisy_fixed']} ({handler_stats['noisy_pct']:.1f}%)\")\n",
        "    \n",
        "    # VECTORIZED: Normalize using TRAINING statistics (critical for proper evaluation)\n",
        "    features = (features - train_stats['mean'].unsqueeze(0)) / train_stats['std'].unsqueeze(0)\n",
        "    \n",
        "    print(f\"\\nFeature stats: mean={features.mean():.4f}, std={features.std():.4f}\")\n",
        "    \n",
        "    # Save cache\n",
        "    torch.save({\n",
        "        'features': features,\n",
        "        'clips': clips,\n",
        "        'videos': videos\n",
        "    }, cache_file)\n",
        "    \n",
        "    # Clean up GPU memory\n",
        "    del i3d\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return features, clips, videos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Simple Autoencoder Model\n",
        "\n",
        "The autoencoder learns to reconstruct normal video features. During inference:\n",
        "- Normal frames: Low reconstruction error\n",
        "- Anomalous frames: High reconstruction error\n",
        "\n",
        "Architecture:\n",
        "- Encoder: 2048 -> 1024 -> 512 -> 128 (latent)\n",
        "- Decoder: 128 -> 512 -> 1024 -> 2048\n",
        "- Uses BatchNorm, ReLU, and Dropout for regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleAutoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple autoencoder with vectorized operations for anomaly detection.\n",
        "    \n",
        "    The autoencoder is trained on normal video features. Anomalous frames\n",
        "    will have higher reconstruction error since the model hasn't learned\n",
        "    to reconstruct abnormal patterns.\n",
        "    \n",
        "    Architecture:\n",
        "        Encoder: input_dim -> hidden_dim -> hidden_dim/2 -> latent_dim\n",
        "        Decoder: latent_dim -> hidden_dim/2 -> hidden_dim -> input_dim\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim: int = 2048, hidden_dim: int = 1024, latent_dim: int = 128):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Encoder: Compress features to latent space\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim // 2, latent_dim),\n",
        "        )\n",
        "        \n",
        "        # Decoder: Reconstruct features from latent space\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "        )\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights using Xavier uniform initialization.\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Fully vectorized forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (B, input_dim)\n",
        "            \n",
        "        Returns:\n",
        "            recon: Reconstructed tensor of shape (B, input_dim)\n",
        "        \"\"\"\n",
        "        return self.decoder(self.encoder(x))\n",
        "    \n",
        "    def compute_loss(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        VECTORIZED: Compute reconstruction and MSE loss.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            \n",
        "        Returns:\n",
        "            recon: Reconstructed tensor\n",
        "            loss: Scalar MSE loss\n",
        "        \"\"\"\n",
        "        recon = self.forward(x)\n",
        "        loss = F.mse_loss(recon, x)\n",
        "        return recon, loss\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def compute_errors_vectorized(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        VECTORIZED: Compute per-sample MSE errors.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (B, D)\n",
        "            \n",
        "        Returns:\n",
        "            errors: Per-sample MSE errors of shape (B,)\n",
        "        \"\"\"\n",
        "        recon = self.forward(x)\n",
        "        # Vectorized MSE per sample: mean over feature dimension\n",
        "        errors = ((x - recon) ** 2).mean(dim=1)\n",
        "        return errors\n",
        "\n",
        "\n",
        "print(\"SimpleAutoencoder class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Pixel-Level Heatmap Visualization (Grad-CAM)\n",
        "\n",
        "Grad-CAM (Gradient-weighted Class Activation Mapping) generates spatial heatmaps showing which regions of the video frames contribute most to the anomaly score.\n",
        "\n",
        "How it works:\n",
        "1. Forward pass through I3D to get intermediate activations\n",
        "2. Backward pass to compute gradients w.r.t. reconstruction error\n",
        "3. Weight activations by gradients to get importance map\n",
        "4. Resize and overlay on original frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GradCAMI3D:\n",
        "    \"\"\"\n",
        "    Grad-CAM for I3D to generate spatial anomaly heatmaps.\n",
        "    \n",
        "    Uses forward and backward hooks to capture activations and gradients\n",
        "    at the last convolutional layer of I3D (blocks[5]).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, i3d_model):\n",
        "        self.model = i3d_model.model\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "        self._register_hooks()\n",
        "    \n",
        "    def _register_hooks(self):\n",
        "        \"\"\"Register forward and backward hooks on the last conv layer.\"\"\"\n",
        "        def forward_hook(module, input, output):\n",
        "            # Store activations (keep reference for gradient computation)\n",
        "            self.activations = output\n",
        "        \n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            # Store gradients\n",
        "            self.gradients = grad_output[0]\n",
        "        \n",
        "        # Hook into last conv layer (blocks[5] for I3D R50, before pooling)\n",
        "        self.model.blocks[5].register_forward_hook(forward_hook)\n",
        "        self.model.blocks[5].register_full_backward_hook(backward_hook)\n",
        "    \n",
        "    def compute_heatmap(self, clip_tensor, autoencoder, train_stats):\n",
        "        \"\"\"\n",
        "        Compute Grad-CAM heatmap for a clip.\n",
        "        \n",
        "        Args:\n",
        "            clip_tensor: (1, C, T, H, W) input tensor\n",
        "            autoencoder: Trained autoencoder model\n",
        "            train_stats: Training normalization statistics\n",
        "            \n",
        "        Returns:\n",
        "            cam: (T, H, W) heatmap for each frame in clip\n",
        "            score: Anomaly score for the clip\n",
        "        \"\"\"\n",
        "        autoencoder.eval()\n",
        "        \n",
        "        # Reset stored activations/gradients\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "        \n",
        "        # Enable gradients for this forward pass\n",
        "        clip_tensor = clip_tensor.clone().requires_grad_(True)\n",
        "        \n",
        "        # Forward through I3D model - hooks will capture activations at blocks[5]\n",
        "        # Temporarily enable gradients for the model\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad_(True)\n",
        "        \n",
        "        features = self.model(clip_tensor)  # (1, 2048)\n",
        "        \n",
        "        # Normalize using training stats\n",
        "        mean = train_stats['mean'].to(features.device)\n",
        "        std = train_stats['std'].to(features.device)\n",
        "        features_norm = (features - mean) / std\n",
        "        \n",
        "        # Get reconstruction error from autoencoder\n",
        "        recon = autoencoder(features_norm)\n",
        "        error = ((features_norm - recon) ** 2).mean()\n",
        "        score = error.item()\n",
        "        \n",
        "        # Backward pass to get gradients\n",
        "        error.backward()\n",
        "        \n",
        "        # Disable gradients again\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        \n",
        "        # Get activations and gradients from hooks (detach for numpy conversion)\n",
        "        activations = self.activations.detach() if self.activations is not None else None\n",
        "        gradients = self.gradients.detach() if self.gradients is not None else None\n",
        "        \n",
        "        if activations is None:\n",
        "            # Fallback: return uniform heatmap\n",
        "            print(\"Warning: No activations captured, using uniform heatmap\")\n",
        "            cam = np.ones((SEQ_LEN, 7, 7), dtype=np.float32)\n",
        "        elif gradients is None:\n",
        "            # Fallback: use activation magnitudes if gradients unavailable\n",
        "            cam = activations.abs().mean(dim=1).squeeze(0).cpu().numpy()\n",
        "        else:\n",
        "            # Grad-CAM: weight activations by average gradients\n",
        "            weights = gradients.mean(dim=(3, 4), keepdim=True)  # (1, C, T, 1, 1)\n",
        "            cam = F.relu((weights * activations).sum(dim=1)).squeeze(0).cpu().numpy()\n",
        "        \n",
        "        return cam, score\n",
        "\n",
        "\n",
        "print(\"GradCAMI3D class defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_anomaly_heatmaps_all(model, train_stats, test_dir):\n",
        "    \"\"\"\n",
        "    Generate pixel-level anomaly heatmaps for ALL test videos.\n",
        "    \n",
        "    Creates visualizations for each video separately.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained autoencoder\n",
        "        train_stats: Training normalization statistics  \n",
        "        test_dir: Path to test videos\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Generating Pixel-Level Heatmaps (ALL VIDEOS)\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Load I3D for Grad-CAM\n",
        "    i3d = I3D().to(device)\n",
        "    gradcam = GradCAMI3D(i3d)\n",
        "    \n",
        "    # Get all video directories\n",
        "    video_dirs = sorted([d for d in os.listdir(test_dir) \n",
        "                        if os.path.isdir(os.path.join(test_dir, d))])\n",
        "    \n",
        "    print(f\"Found {len(video_dirs)} videos\")\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # Loop through each video\n",
        "    for vid_idx, vid_name in enumerate(video_dirs, 1):\n",
        "        print(f\"\\n[{vid_idx}/{len(video_dirs)}] Processing: {vid_name}\")\n",
        "        \n",
        "        vid_path = os.path.join(test_dir, vid_name)\n",
        "        frames = sorted(glob.glob(os.path.join(vid_path, \"*.jpg\")))\n",
        "        \n",
        "        if len(frames) < SEQ_LEN:\n",
        "            print(f\"  Skipped: Not enough frames ({len(frames)} < {SEQ_LEN})\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"  Frames: {len(frames)}\")\n",
        "        \n",
        "        # Select evenly spaced frames\n",
        "        n_total = len(frames)\n",
        "        indices = np.linspace(0, n_total - SEQ_LEN, HEATMAP_FRAMES, dtype=int)\n",
        "        \n",
        "        # Storage for visualization\n",
        "        orig_frames = []\n",
        "        heatmaps = []\n",
        "        scores = []\n",
        "        frame_nums = []\n",
        "        \n",
        "        for idx in tqdm(indices, desc=f\"  Computing heatmaps\", leave=False):\n",
        "            # Load clip\n",
        "            clip_paths = frames[idx:idx + SEQ_LEN]\n",
        "            clip_data = load_clip_vectorized(clip_paths)\n",
        "            \n",
        "            # Get center frame for display\n",
        "            center_idx = SEQ_LEN // 2\n",
        "            center_frame_path = clip_paths[center_idx]\n",
        "            orig_img = cv2.imread(center_frame_path)\n",
        "            orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
        "            orig_img = cv2.resize(orig_img, (IMG_SIZE, IMG_SIZE))\n",
        "            orig_frames.append(orig_img)\n",
        "            frame_nums.append(idx)\n",
        "            \n",
        "            # Prepare tensor: (1, C, T, H, W)\n",
        "            clip_tensor = torch.from_numpy(clip_data).unsqueeze(0).permute(0, 4, 1, 2, 3)\n",
        "            clip_tensor = clip_tensor.to(device)\n",
        "            \n",
        "            # Compute Grad-CAM heatmap\n",
        "            with torch.enable_grad():\n",
        "                cam, score = gradcam.compute_heatmap(clip_tensor, model, train_stats)\n",
        "            \n",
        "            # Take center frame heatmap (use actual cam length to handle dimension mismatches)\n",
        "            cam_center_idx = len(cam) // 2 if len(cam) > 0 else 0\n",
        "            cam_frame = cam[cam_center_idx]\n",
        "            \n",
        "            # Resize heatmap to original image size\n",
        "            cam_resized = cv2.resize(cam_frame, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n",
        "            \n",
        "            # Normalize heatmap to [0, 1]\n",
        "            if cam_resized.max() > cam_resized.min():\n",
        "                cam_resized = (cam_resized - cam_resized.min()) / (cam_resized.max() - cam_resized.min())\n",
        "            \n",
        "            heatmaps.append(cam_resized)\n",
        "            scores.append(score)\n",
        "        \n",
        "        # Normalize scores across all frames for display\n",
        "        scores = np.array(scores)\n",
        "        if scores.max() > scores.min():\n",
        "            scores_norm = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "        else:\n",
        "            scores_norm = scores\n",
        "        \n",
        "        # Create visualization 1: Pure heatmaps\n",
        "        output_path = f\"pixel_heatmaps_video{vid_idx:02d}.png\"\n",
        "        fig, axes = plt.subplots(2, HEATMAP_FRAMES, figsize=(HEATMAP_FRAMES * 2.5, 5))\n",
        "        fig.suptitle(f'Pixel-Level Anomaly Heatmaps - {vid_name}', fontsize=14, fontweight='bold')\n",
        "        \n",
        "        for i in range(HEATMAP_FRAMES):\n",
        "            # Top row: Original frames\n",
        "            axes[0, i].imshow(orig_frames[i])\n",
        "            axes[0, i].set_title(f'Frame {frame_nums[i]}', fontsize=10)\n",
        "            axes[0, i].axis('off')\n",
        "            \n",
        "            # Bottom row: Pure heatmaps\n",
        "            axes[1, i].imshow(heatmaps[i], cmap='hot', vmin=0, vmax=1)\n",
        "            axes[1, i].set_title(f'{scores_norm[i]:.3f}', fontsize=10)\n",
        "            axes[1, i].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"  Saved: {output_path}\")\n",
        "        \n",
        "        # Create visualization 2: Overlay version\n",
        "        overlay_path = output_path.replace('.png', '_overlay.png')\n",
        "        fig2, axes2 = plt.subplots(2, HEATMAP_FRAMES, figsize=(HEATMAP_FRAMES * 2.5, 5))\n",
        "        fig2.suptitle(f'Pixel-Level Anomaly Heatmaps (Overlay) - {vid_name}', fontsize=14, fontweight='bold')\n",
        "        \n",
        "        for i in range(HEATMAP_FRAMES):\n",
        "            # Top row: Original frames\n",
        "            axes2[0, i].imshow(orig_frames[i])\n",
        "            axes2[0, i].set_title(f'Frame {frame_nums[i]}', fontsize=10)\n",
        "            axes2[0, i].axis('off')\n",
        "            \n",
        "            # Bottom row: Heatmap overlaid on original\n",
        "            heatmap_colored = cv2.applyColorMap(\n",
        "                (heatmaps[i] * 255).astype(np.uint8), \n",
        "                cv2.COLORMAP_JET\n",
        "            )\n",
        "            heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            # Blend with original\n",
        "            alpha = 0.5\n",
        "            overlay = cv2.addWeighted(orig_frames[i], 1-alpha, heatmap_colored, alpha, 0)\n",
        "            \n",
        "            axes2[1, i].imshow(overlay)\n",
        "            axes2[1, i].set_title(f'{scores_norm[i]:.3f}', fontsize=10)\n",
        "            axes2[1, i].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(overlay_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"  Saved: {overlay_path}\")\n",
        "\n",
        "\n",
        "print(\"visualize_anomaly_heatmaps_all function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. t-SNE Visualization\n",
        "\n",
        "t-SNE (t-Distributed Stochastic Neighbor Embedding) projects high-dimensional features into 2D for visualization. This helps understand how well normal and anomalous frames are separated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_tsne(features, scores, threshold, output_path=\"tsne_visualization.png\",\n",
        "                   perplexity=30, max_samples=2000):\n",
        "    \"\"\"\n",
        "    Generate t-SNE 2D visualization of features colored by anomaly status.\n",
        "    \n",
        "    Creates two plots:\n",
        "    1. Binary coloring: Normal (blue) vs Anomaly (red)\n",
        "    2. Continuous coloring: Gradient based on anomaly score\n",
        "    \n",
        "    Args:\n",
        "        features: (N, D) feature tensor\n",
        "        scores: (N,) normalized anomaly scores  \n",
        "        threshold: Anomaly threshold value\n",
        "        output_path: Output image path\n",
        "        perplexity: t-SNE perplexity parameter\n",
        "        max_samples: Maximum samples to visualize (for speed)\n",
        "    \"\"\"\n",
        "    from sklearn.manifold import TSNE\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Generating t-SNE Visualization\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Convert to numpy if tensor\n",
        "    if isinstance(features, torch.Tensor):\n",
        "        features_np = features.numpy()\n",
        "    else:\n",
        "        features_np = features\n",
        "    \n",
        "    if isinstance(scores, torch.Tensor):\n",
        "        scores_np = scores.numpy()\n",
        "    else:\n",
        "        scores_np = np.array(scores)\n",
        "    \n",
        "    n_samples = len(features_np)\n",
        "    print(f\"Total samples: {n_samples}\")\n",
        "    \n",
        "    # Subsample if too many points (t-SNE is slow for large datasets)\n",
        "    if n_samples > max_samples:\n",
        "        print(f\"Subsampling to {max_samples} samples for t-SNE...\")\n",
        "        indices = np.random.choice(n_samples, max_samples, replace=False)\n",
        "        indices = np.sort(indices)\n",
        "        features_np = features_np[indices]\n",
        "        scores_np = scores_np[indices]\n",
        "        n_samples = max_samples\n",
        "    \n",
        "    # Classify as normal/anomalous based on threshold\n",
        "    is_anomaly = scores_np > threshold\n",
        "    n_anomalies = is_anomaly.sum()\n",
        "    n_normal = n_samples - n_anomalies\n",
        "    \n",
        "    print(f\"Normal: {n_normal}, Anomalous: {n_anomalies}\")\n",
        "    \n",
        "    # Run t-SNE\n",
        "    print(f\"Running t-SNE (perplexity={perplexity})...\")\n",
        "    tsne = TSNE(\n",
        "        n_components=2,\n",
        "        perplexity=min(perplexity, n_samples - 1),\n",
        "        random_state=SEED,\n",
        "        n_iter=1000,\n",
        "        learning_rate='auto',\n",
        "        init='pca'\n",
        "    )\n",
        "    \n",
        "    embeddings = tsne.fit_transform(features_np)\n",
        "    print(\"t-SNE complete!\")\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "    \n",
        "    # Plot 1: Binary coloring (Normal vs Anomaly)\n",
        "    ax1 = axes[0]\n",
        "    normal_mask = ~is_anomaly\n",
        "    \n",
        "    # Plot normal points first (so anomalies are on top)\n",
        "    ax1.scatter(embeddings[normal_mask, 0], embeddings[normal_mask, 1],\n",
        "                c='#2E86AB', s=15, alpha=0.6, label=f'Normal ({n_normal})')\n",
        "    ax1.scatter(embeddings[is_anomaly, 0], embeddings[is_anomaly, 1],\n",
        "                c='#E63946', s=25, alpha=0.8, label=f'Anomaly ({n_anomalies})')\n",
        "    \n",
        "    ax1.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
        "    ax1.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
        "    ax1.set_title('t-SNE: Normal vs Anomalous Frames', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(loc='best', fontsize=11)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Continuous coloring by anomaly score\n",
        "    ax2 = axes[1]\n",
        "    scatter = ax2.scatter(embeddings[:, 0], embeddings[:, 1],\n",
        "                          c=scores_np, cmap='RdYlGn_r', s=15, alpha=0.7)\n",
        "    \n",
        "    # Add colorbar with threshold line\n",
        "    cbar = plt.colorbar(scatter, ax=ax2)\n",
        "    cbar.set_label('Anomaly Score', fontsize=12)\n",
        "    cbar.ax.axhline(y=threshold, color='black', linewidth=2, linestyle='--')\n",
        "    \n",
        "    ax2.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
        "    ax2.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
        "    ax2.set_title('t-SNE: Colored by Anomaly Score', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved: {output_path}\")\n",
        "    \n",
        "    # Print embedding statistics\n",
        "    print(f\"\\nEmbedding Statistics:\")\n",
        "    print(f\"  Normal centroid:  ({embeddings[normal_mask, 0].mean():.2f}, {embeddings[normal_mask, 1].mean():.2f})\")\n",
        "    if n_anomalies > 0:\n",
        "        print(f\"  Anomaly centroid: ({embeddings[is_anomaly, 0].mean():.2f}, {embeddings[is_anomaly, 1].mean():.2f})\")\n",
        "    \n",
        "    return embeddings, is_anomaly\n",
        "\n",
        "\n",
        "print(\"visualize_tsne function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Function\n",
        "\n",
        "Train the autoencoder on normal video features extracted by I3D.\n",
        "\n",
        "Training details:\n",
        "- Optimizer: AdamW with weight decay\n",
        "- Scheduler: Cosine annealing learning rate\n",
        "- Loss: Mean Squared Error (MSE)\n",
        "- Best model is saved based on lowest training loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train():\n",
        "    \"\"\"\n",
        "    Train the autoencoder on normal video features.\n",
        "    \n",
        "    Steps:\n",
        "    1. Extract I3D features from training videos\n",
        "    2. Create autoencoder model\n",
        "    3. Train with MSE loss\n",
        "    4. Save best model and training statistics\n",
        "    5. Plot training curve\n",
        "    \n",
        "    Returns:\n",
        "        model: Trained autoencoder\n",
        "        stats: Training feature statistics (mean, std)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"  TRAINING (Vectorized)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Extract I3D features from training videos\n",
        "    features, clips, videos, stats = extract_features_vectorized(\n",
        "        TRAIN_DIR, \"train\", stride=2  # stride=2 for data augmentation\n",
        "    )\n",
        "    \n",
        "    n_samples = len(features)\n",
        "    print(f\"\\nTraining samples: {n_samples}\")\n",
        "    \n",
        "    # VECTORIZED: Use TensorDataset for efficient batching\n",
        "    train_dataset = TensorDataset(features)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=TRAIN_BATCH,\n",
        "        shuffle=True,\n",
        "        num_workers=0,  # Data already in memory\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    \n",
        "    # Create model\n",
        "    model = SimpleAutoencoder(input_dim=2048, hidden_dim=1024, latent_dim=128).to(device)\n",
        "    \n",
        "    # Skip torch.compile for older GPUs (Tesla P100, V100, etc.)\n",
        "    # torch.compile requires CUDA 7.0+, but P100 has CUDA 6.0\n",
        "    print(\"Skipping torch.compile() - not supported on this GPU\")\n",
        "    \n",
        "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Parameters: {n_params:,}\")\n",
        "    \n",
        "    # Optimizer with weight decay for regularization\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    \n",
        "    # Cosine annealing scheduler for smooth learning rate decay\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
        "    \n",
        "    print(f\"\\nTraining for {NUM_EPOCHS} epochs...\")\n",
        "    \n",
        "    best_loss = float('inf')\n",
        "    losses = []\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        n_batches = 0\n",
        "        \n",
        "        for (batch,) in train_loader:\n",
        "            batch = batch.to(device, non_blocking=True)\n",
        "            \n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            \n",
        "            # VECTORIZED: Forward + loss in one call\n",
        "            _, loss = model.compute_loss(batch)\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            n_batches += 1\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        avg_loss = epoch_loss / n_batches\n",
        "        losses.append(avg_loss)\n",
        "        \n",
        "        # Print progress every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS} | Loss: {avg_loss:.6f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "        \n",
        "        # Save best model\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), os.path.join(FEATURE_CACHE, 'best_model.pth'))\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(os.path.join(FEATURE_CACHE, 'best_model.pth'), weights_only=True))\n",
        "    print(f\"\\nBest loss: {best_loss:.6f}\")\n",
        "    \n",
        "    # Save training statistics for later use\n",
        "    torch.save(stats, os.path.join(FEATURE_CACHE, 'train_stats.pth'))\n",
        "    print(f\"Saved training stats to {os.path.join(FEATURE_CACHE, 'train_stats.pth')}\")\n",
        "    \n",
        "    # Plot training curve\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(losses, 'b-', linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('training_loss.png', dpi=150)\n",
        "    plt.show()\n",
        "    \n",
        "    return model, stats\n",
        "\n",
        "\n",
        "print(\"train function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation Function\n",
        "\n",
        "Evaluate the trained model on test videos and compute anomaly scores.\n",
        "\n",
        "Steps:\n",
        "1. Extract I3D features from test videos\n",
        "2. Compute reconstruction errors\n",
        "3. Aggregate scores per frame with temporal smoothing\n",
        "4. Compute AUC if ground truth is available\n",
        "5. Generate visualizations (anomaly plots, heatmaps, t-SNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, train_stats):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on test videos and save scores.\n",
        "    \n",
        "    This is a simplified evaluation that computes anomaly scores\n",
        "    and saves them to CSV. Use separate functions for visualizations:\n",
        "    - generate_heatmaps_independent() for pixel-level heatmaps\n",
        "    - generate_tsne_independent() for t-SNE visualization\n",
        "    \n",
        "    Args:\n",
        "        model: Trained autoencoder\n",
        "        train_stats: Training feature statistics for normalization\n",
        "        \n",
        "    Returns:\n",
        "        all_scores_norm: Normalized anomaly scores per frame\n",
        "        threshold: Computed anomaly threshold\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"  EVALUATION (Vectorized)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Extract test features using TRAINING normalization stats\n",
        "    features, clips, videos = extract_test_features_vectorized(\n",
        "        TEST_DIR, train_stats, stride=1\n",
        "    )\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    n_clips = len(features)\n",
        "    print(f\"\\nTest clips: {n_clips}\")\n",
        "    \n",
        "    # VECTORIZED: Compute all errors in batches\n",
        "    all_errors = torch.zeros(n_clips, dtype=torch.float32)\n",
        "    \n",
        "    print(\"Computing reconstruction errors...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, n_clips, TRAIN_BATCH * 2), desc=\"Evaluating\"):\n",
        "            batch = features[i:i + TRAIN_BATCH * 2].to(device)\n",
        "            \n",
        "            # VECTORIZED: Get per-sample errors\n",
        "            errors = model.compute_errors_vectorized(batch)\n",
        "            all_errors[i:i + len(errors)] = errors.cpu()\n",
        "    \n",
        "    errors_np = all_errors.numpy()\n",
        "    \n",
        "    # VECTORIZED: Build frame score mapping\n",
        "    print(\"Aggregating frame scores...\")\n",
        "    \n",
        "    video_scores = {}\n",
        "    video_frame_ids = {}\n",
        "    frame_error_map = defaultdict(list)\n",
        "    \n",
        "    # Map clip errors to frames (center frames for accuracy)\n",
        "    for i, clip in enumerate(clips):\n",
        "        vid_id = clip['vid']\n",
        "        frame_nums = clip['nums']\n",
        "        error = errors_np[i]\n",
        "        \n",
        "        # Assign to center frames (more accurate)\n",
        "        mid_start = SEQ_LEN // 4\n",
        "        mid_end = SEQ_LEN - SEQ_LEN // 4\n",
        "        \n",
        "        for j in range(mid_start, mid_end):\n",
        "            frame_error_map[(vid_id, frame_nums[j])].append(error)\n",
        "    \n",
        "    # VECTORIZED: Aggregate scores per video\n",
        "    all_frame_ids = []\n",
        "    all_scores = []\n",
        "    \n",
        "    for vid_id, info in sorted(videos.items()):\n",
        "        n_frames = info['n']\n",
        "        frame_nums = info['nums']\n",
        "        \n",
        "        # Pre-allocate score array\n",
        "        scores = np.zeros(n_frames, dtype=np.float32)\n",
        "        \n",
        "        for i, fn in enumerate(frame_nums):\n",
        "            key = (vid_id, fn)\n",
        "            if key in frame_error_map:\n",
        "                scores[i] = np.max(frame_error_map[key])  # MAX for anomalies\n",
        "        \n",
        "        # VECTORIZED: Temporal smoothing using Gaussian filter\n",
        "        from scipy.ndimage import gaussian_filter1d\n",
        "        scores = gaussian_filter1d(scores.astype(np.float64), sigma=3)\n",
        "        \n",
        "        video_scores[vid_id] = scores\n",
        "        \n",
        "        # Build frame IDs\n",
        "        frame_ids = [f\"{vid_id}_{fn}\" for fn in frame_nums]\n",
        "        video_frame_ids[vid_id] = frame_ids\n",
        "        \n",
        "        all_frame_ids.extend(frame_ids)\n",
        "        all_scores.extend(scores)\n",
        "    \n",
        "    # VECTORIZED: Convert to numpy array\n",
        "    all_scores = np.array(all_scores, dtype=np.float64)\n",
        "    \n",
        "    # VECTORIZED: Normalize using percentiles (robust to outliers)\n",
        "    p1, p99 = np.percentile(all_scores, [1, 99])\n",
        "    all_scores_norm = np.clip(all_scores, p1, p99)\n",
        "    all_scores_norm = (all_scores_norm - p1) / (p99 - p1 + 1e-8)\n",
        "    \n",
        "    # Compute threshold (mean + 2*std)\n",
        "    threshold = all_scores_norm.mean() + 2 * all_scores_norm.std()\n",
        "    n_anomalies = (all_scores_norm > threshold).sum()\n",
        "    \n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  Frames: {len(all_scores_norm)}\")\n",
        "    print(f\"  Score range: [{all_scores_norm.min():.4f}, {all_scores_norm.max():.4f}]\")\n",
        "    print(f\"  Threshold: {threshold:.4f}\")\n",
        "    print(f\"  Anomalies: {n_anomalies} ({100*n_anomalies/len(all_scores_norm):.1f}%)\")\n",
        "    \n",
        "    # Save results to CSV\n",
        "    df = pd.DataFrame({'Id': all_frame_ids, 'Score': all_scores_norm})\n",
        "    df.to_csv(OUTPUT_CSV, index=False)\n",
        "    print(f\"\\nSaved: {OUTPUT_CSV}\")\n",
        "    \n",
        "    # Try to compute AUC if ground truth is available\n",
        "    try:\n",
        "        from sklearn.metrics import roc_auc_score\n",
        "        from scipy.io import loadmat\n",
        "        \n",
        "        gt_paths = [\n",
        "            \"/kaggle/input/avenue-dataset/Avenue_Corrupted/testing_label\",\n",
        "            \"/kaggle/input/avenue-dataset/testing_label\",\n",
        "            os.path.join(DATA_ROOT, \"..\", \"testing_label\"),\n",
        "        ]\n",
        "        \n",
        "        gt_dir = None\n",
        "        for path in gt_paths:\n",
        "            if os.path.exists(path):\n",
        "                gt_dir = path\n",
        "                break\n",
        "        \n",
        "        if gt_dir:\n",
        "            print(f\"\\nComputing AUC (GT: {gt_dir})\")\n",
        "            \n",
        "            aucs = []\n",
        "            for vid_id, scores in video_scores.items():\n",
        "                mat_files = glob.glob(os.path.join(gt_dir, f\"*{vid_id}*.mat\"))\n",
        "                if not mat_files:\n",
        "                    mat_files = glob.glob(os.path.join(gt_dir, f\"*{vid_id:02d}*.mat\"))\n",
        "                \n",
        "                if mat_files:\n",
        "                    mat = loadmat(mat_files[0])\n",
        "                    for key in mat:\n",
        "                        if not key.startswith('__'):\n",
        "                            gt = np.array(mat[key]).flatten()\n",
        "                            break\n",
        "                    \n",
        "                    # Normalize scores\n",
        "                    s = scores.copy()\n",
        "                    if s.max() > s.min():\n",
        "                        s = (s - s.min()) / (s.max() - s.min())\n",
        "                    \n",
        "                    min_len = min(len(s), len(gt))\n",
        "                    s, gt = s[:min_len], gt[:min_len]\n",
        "                    \n",
        "                    if len(np.unique(gt)) > 1:\n",
        "                        auc = roc_auc_score(gt, s)\n",
        "                        aucs.append(auc)\n",
        "                        print(f\"  Video {vid_id}: AUC = {auc:.4f}\")\n",
        "            \n",
        "            if aucs:\n",
        "                print(f\"\\n{'='*40}\")\n",
        "                print(f\"  MEAN AUC: {np.mean(aucs):.4f}\")\n",
        "                print(f\"{'='*40}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not compute AUC: {e}\")\n",
        "    \n",
        "    # Visualization: Anomaly scores plot\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
        "    \n",
        "    axes[0].plot(all_scores_norm, color='#E63946', linewidth=0.8)\n",
        "    axes[0].axhline(threshold, color='#F4A261', linestyle='--', linewidth=2)\n",
        "    axes[0].fill_between(range(len(all_scores_norm)), all_scores_norm, threshold,\n",
        "                          where=all_scores_norm > threshold, alpha=0.4, color='#E63946')\n",
        "    axes[0].set_title(\"Anomaly Scores (I3D + Autoencoder)\", fontsize=14)\n",
        "    axes[0].set_xlabel(\"Frame\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[1].hist(all_scores_norm, bins=100, color='#457B9D', alpha=0.7)\n",
        "    axes[1].axvline(threshold, color='#E63946', linestyle='--', linewidth=2)\n",
        "    axes[1].set_title(\"Score Distribution\")\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('anomaly_analysis.png', dpi=150)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nTo generate additional visualizations, run:\")\n",
        "    print(\"  - generate_heatmaps_independent() for pixel-level heatmaps\")\n",
        "    print(\"  - generate_tsne_independent() for t-SNE visualization\")\n",
        "    \n",
        "    return all_scores_norm, threshold\n",
        "\n",
        "\n",
        "print(\"evaluate function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10a. Heatmap Generation (Independent Cell)\n",
        "\n",
        "Generate pixel-level anomaly heatmaps independently. Loads pre-trained model and training stats.\n",
        "This cell can be run separately without re-training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_heatmaps_independent():\n",
        "    \"\"\"\n",
        "    Generate pixel-level anomaly heatmaps for ALL VIDEOS INDEPENDENTLY.\n",
        "    \n",
        "    Loads pre-trained model and training statistics from disk.\n",
        "    Can be run without re-training.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"  HEATMAP GENERATION (Independent - ALL VIDEOS)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Load pre-trained model\n",
        "    model_path = os.path.join(FEATURE_CACHE, 'best_model.pth')\n",
        "    stats_path = os.path.join(FEATURE_CACHE, 'train_stats.pth')\n",
        "    \n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"ERROR: Model not found at {model_path}\")\n",
        "        print(\"Please run training first!\")\n",
        "        return\n",
        "    \n",
        "    if not os.path.exists(stats_path):\n",
        "        print(f\"ERROR: Training stats not found at {stats_path}\")\n",
        "        print(\"Please run training first!\")\n",
        "        return\n",
        "    \n",
        "    # Load model\n",
        "    print(f\"Loading model from {model_path}...\")\n",
        "    model = SimpleAutoencoder(input_dim=2048, hidden_dim=1024, latent_dim=128).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
        "    model.eval()\n",
        "    \n",
        "    # Load training stats\n",
        "    print(f\"Loading training stats from {stats_path}...\")\n",
        "    train_stats = torch.load(stats_path, weights_only=False)\n",
        "    \n",
        "    # Generate heatmaps for ALL videos\n",
        "    visualize_anomaly_heatmaps_all(model, train_stats, TEST_DIR)\n",
        "    \n",
        "    print(\"\\nHeatmap generation complete!\")\n",
        "    print(\"Outputs: pixel_heatmaps_video*.png and pixel_heatmaps_video*_overlay.png\")\n",
        "\n",
        "\n",
        "print(\"generate_heatmaps_independent function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10b. t-SNE Generation (Independent Cell)\n",
        "\n",
        "Generate t-SNE visualization independently. Loads test features and trained model.\n",
        "This cell can be run separately without re-training or re-extracting features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_tsne_independent():\n",
        "    \"\"\"\n",
        "    Generate t-SNE visualization INDEPENDENTLY.\n",
        "    \n",
        "    Loads pre-trained model, training statistics, and test features from disk.\n",
        "    Can be run without re-training or re-extracting features.\n",
        "    \n",
        "    This function is self-contained and handles:\n",
        "    1. Loading model and training stats\n",
        "    2. Extracting test features\n",
        "    3. Computing reconstruction errors\n",
        "    4. Generating t-SNE visualization\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"  t-SNE GENERATION (Independent)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Load pre-trained model\n",
        "    model_path = os.path.join(FEATURE_CACHE, 'best_model.pth')\n",
        "    stats_path = os.path.join(FEATURE_CACHE, 'train_stats.pth')\n",
        "    \n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"ERROR: Model not found at {model_path}\")\n",
        "        print(\"Please run training first!\")\n",
        "        return\n",
        "    \n",
        "    if not os.path.exists(stats_path):\n",
        "        print(f\"ERROR: Training stats not found at {stats_path}\")\n",
        "        print(\"Please run training first!\")\n",
        "        return\n",
        "    \n",
        "    # Load model\n",
        "    print(f\"Loading model from {model_path}...\")\n",
        "    model = SimpleAutoencoder(input_dim=2048, hidden_dim=1024, latent_dim=128).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
        "    model.eval()\n",
        "    \n",
        "    # Load training stats\n",
        "    print(f\"Loading training stats from {stats_path}...\")\n",
        "    train_stats = torch.load(stats_path, weights_only=False)\n",
        "    \n",
        "    # Extract test features\n",
        "    print(\"\\nExtracting test features...\")\n",
        "    features, clips, videos = extract_test_features_vectorized(\n",
        "        TEST_DIR, train_stats, stride=1\n",
        "    )\n",
        "    \n",
        "    n_clips = len(features)\n",
        "    print(f\"Test clips: {n_clips}\")\n",
        "    \n",
        "    # VECTORIZED: Compute all reconstruction errors in batches\n",
        "    print(\"Computing reconstruction errors...\")\n",
        "    all_errors = torch.zeros(n_clips, dtype=torch.float32)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, n_clips, TRAIN_BATCH * 2), desc=\"Evaluating\"):\n",
        "            batch = features[i:i + TRAIN_BATCH * 2].to(device)\n",
        "            errors = model.compute_errors_vectorized(batch)\n",
        "            all_errors[i:i + len(errors)] = errors.cpu()\n",
        "    \n",
        "    errors_np = all_errors.numpy()\n",
        "    \n",
        "    # VECTORIZED: Build frame score mapping\n",
        "    print(\"Aggregating frame scores...\")\n",
        "    frame_error_map = defaultdict(list)\n",
        "    \n",
        "    for i, clip in enumerate(clips):\n",
        "        vid_id = clip['vid']\n",
        "        frame_nums = clip['nums']\n",
        "        error = errors_np[i]\n",
        "        \n",
        "        mid_start = SEQ_LEN // 4\n",
        "        mid_end = SEQ_LEN - SEQ_LEN // 4\n",
        "        \n",
        "        for j in range(mid_start, mid_end):\n",
        "            frame_error_map[(vid_id, frame_nums[j])].append(error)\n",
        "    \n",
        "    # VECTORIZED: Aggregate scores per video\n",
        "    all_scores = []\n",
        "    \n",
        "    for vid_id, info in sorted(videos.items()):\n",
        "        n_frames = info['n']\n",
        "        frame_nums = info['nums']\n",
        "        \n",
        "        scores = np.zeros(n_frames, dtype=np.float32)\n",
        "        \n",
        "        for i, fn in enumerate(frame_nums):\n",
        "            key = (vid_id, fn)\n",
        "            if key in frame_error_map:\n",
        "                scores[i] = np.max(frame_error_map[key])\n",
        "        \n",
        "        from scipy.ndimage import gaussian_filter1d\n",
        "        scores = gaussian_filter1d(scores.astype(np.float64), sigma=3)\n",
        "        \n",
        "        all_scores.extend(scores)\n",
        "    \n",
        "    # VECTORIZED: Convert to numpy array\n",
        "    all_scores = np.array(all_scores, dtype=np.float64)\n",
        "    \n",
        "    # VECTORIZED: Normalize using percentiles\n",
        "    p1, p99 = np.percentile(all_scores, [1, 99])\n",
        "    all_scores_norm = np.clip(all_scores, p1, p99)\n",
        "    all_scores_norm = (all_scores_norm - p1) / (p99 - p1 + 1e-8)\n",
        "    \n",
        "    # Compute threshold\n",
        "    threshold = all_scores_norm.mean() + 2 * all_scores_norm.std()\n",
        "    \n",
        "    print(f\"\\nScore Statistics:\")\n",
        "    print(f\"  Frames: {len(all_scores_norm)}\")\n",
        "    print(f\"  Range: [{all_scores_norm.min():.4f}, {all_scores_norm.max():.4f}]\")\n",
        "    print(f\"  Threshold: {threshold:.4f}\")\n",
        "    \n",
        "    # Generate t-SNE visualization\n",
        "    print(\"\\nGenerating t-SNE visualization...\")\n",
        "    visualize_tsne(\n",
        "        features, all_scores_norm, threshold,\n",
        "        output_path='tsne_visualization.png',\n",
        "        perplexity=TSNE_PERPLEXITY,\n",
        "        max_samples=TSNE_MAX_SAMPLES\n",
        "    )\n",
        "    \n",
        "    print(\"t-SNE generation complete!\")\n",
        "    print(\"Output: tsne_visualization.png\")\n",
        "\n",
        "\n",
        "print(\"generate_tsne_independent function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Run the Pipeline\n",
        "\n",
        "Execute the complete anomaly detection pipeline:\n",
        "1. Check dependencies\n",
        "2. Train the autoencoder\n",
        "3. Evaluate on test data\n",
        "4. Generate visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MAIN EXECUTION - TRAINING & EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"  AVENUE ANOMALY DETECTION - VECTORIZED\")\n",
        "print(\"  I3D + Simple Autoencoder + Corruption Handling\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Check and install dependencies if needed\n",
        "try:\n",
        "    import pytorchvideo\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    print(\"\\n[OK] Dependencies verified\")\n",
        "except ImportError:\n",
        "    print(\"\\n[INFO] Installing missing dependencies...\")\n",
        "    os.system(\"pip install pytorchvideo scikit-learn -q\")\n",
        "\n",
        "# Step 1: Train the autoencoder\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"Step 1: Training\")\n",
        "print(\"-\" * 40)\n",
        "model, train_stats = train()\n",
        "\n",
        "# Step 2: Evaluate on test data (with corruption correction)\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"Step 2: Evaluation (with Corruption Correction)\")\n",
        "print(\"-\" * 40)\n",
        "scores, threshold = evaluate(model, train_stats)\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"  Done! Total time: {(time.time() - start)/60:.1f} min\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nOptimizations applied:\")\n",
        "print(\"  âœ“ Parallel image loading (ThreadPoolExecutor)\")\n",
        "print(\"  âœ“ Batch normalization with broadcasting\")\n",
        "print(\"  âœ“ Pre-allocated tensors for features\")\n",
        "print(\"  âœ“ TensorDataset for efficient batching\")\n",
        "print(\"  âœ“ Vectorized MSE computation\")\n",
        "print(\"  âœ“ Vectorized score aggregation\")\n",
        "\n",
        "print(\"\\nCorruption handling (test data only):\")\n",
        "print(\"  âœ“ Automatic noise detection (Laplacian variance)\")\n",
        "print(\"  âœ“ Noise removal (Gaussian blur)\")\n",
        "print(\"  âœ“ Inversion detection (brightness comparison)\")\n",
        "print(\"  âœ“ Inversion correction (vertical flip)\")\n",
        "\n",
        "print(\"\\nOutputs generated:\")\n",
        "print(\"  - training_loss.png: Training loss curve\")\n",
        "print(\"  - anomaly_analysis.png: Anomaly scores and distribution\")\n",
        "print(\"  - avenue_scores.csv: Frame-level anomaly scores\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Independent Visualization Functions Available:\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nRun heatmap generation (separate cell):\")\n",
        "print(\"  generate_heatmaps_independent()\")\n",
        "print(\"\\nRun t-SNE generation (separate cell):\")\n",
        "print(\"  generate_tsne_independent()\")\n",
        "print(\"\\nThese functions can be called independently after training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook implements a complete video anomaly detection pipeline with automatic corruption handling:\n",
        "\n",
        "### Architecture\n",
        "- **Feature Extractor**: I3D (Inflated 3D ConvNet) pretrained on Kinetics\n",
        "- **Corruption Handler**: Automatic detection and correction of noisy/inverted test images\n",
        "- **Anomaly Detector**: Simple autoencoder trained on normal features\n",
        "- **Scoring**: MSE-based reconstruction error with temporal smoothing\n",
        "\n",
        "### Corruption Handling (Test Only)\n",
        "The pipeline includes automatic detection and correction of common image corruptions:\n",
        "\n",
        "| Feature | Detection | Correction |\n",
        "|---------|-----------|------------|\n",
        "| **Noise** | Laplacian variance > 800 | Gaussian blur (5Ã—5) |\n",
        "| **Inversion** | Lower brightness > Upper + 25 | Vertical flip |\n",
        "\n",
        "- Applied only during test feature extraction\n",
        "- Thread-safe statistics tracking\n",
        "- Fast processing using downsampled images (64Ã—64) for detection\n",
        "\n",
        "### Visualizations\n",
        "1. **Training Loss Curve**: Monitor convergence\n",
        "2. **Anomaly Score Plot**: Frame-level scores with threshold\n",
        "3. **Score Distribution**: Histogram of anomaly scores\n",
        "4. **Pixel-Level Heatmaps**: Grad-CAM visualization showing spatial anomalies\n",
        "5. **t-SNE Embedding**: 2D visualization of feature space\n",
        "\n",
        "### Configuration Options\n",
        "- Toggle heatmaps: `ENABLE_HEATMAPS = True/False`\n",
        "- Toggle t-SNE: `ENABLE_TSNE = True/False`\n",
        "- Adjust training: `NUM_EPOCHS`, `LEARNING_RATE`, `TRAIN_BATCH`\n",
        "- Corruption thresholds: Modify `CorruptionHandlerFast` parameters\n",
        "\n",
        "### References\n",
        "- I3D Paper: \"Quo Vadis, Action Recognition?\" (Carreira & Zisserman, 2017)\n",
        "- Avenue Dataset: \"Abnormal Event Detection at 150 FPS in MATLAB\" (Lu et al., 2013)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Run Heatmap Generation (Optional - Independent)\n",
        "\n",
        "Execute this cell to generate pixel-level anomaly heatmaps.\n",
        "Requires training to be completed first (model.pth must exist).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXECUTE HEATMAP GENERATION (INDEPENDENT)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nStarting heatmap generation...\")\n",
        "generate_heatmaps_independent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Run t-SNE Generation (Optional - Independent)\n",
        "\n",
        "Execute this cell to generate t-SNE visualization.\n",
        "Requires training to be completed first (model.pth must exist).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXECUTE t-SNE GENERATION (INDEPENDENT)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nStarting t-SNE generation...\")\n",
        "generate_tsne_independent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“‹ Notebook Structure Summary\n",
        "\n",
        "This notebook is now organized into **independent components** that can be run separately:\n",
        "\n",
        "### Main Pipeline\n",
        "1. **Section 11**: Training + Evaluation\n",
        "   - Trains the autoencoder on normal video features\n",
        "   - **Applies corruption correction** (noise removal + inversion fix) during test evaluation\n",
        "   - Saves model to `best_model.pth`\n",
        "   - Saves training stats to `train_stats.pth`\n",
        "   - Generates anomaly scores and basic plot\n",
        "   - **Run this first**\n",
        "\n",
        "### Optional Independent Visualizations\n",
        "2. **Section 12**: Heatmap Generation\n",
        "   - Loads pre-trained model\n",
        "   - Generates pixel-level Grad-CAM heatmaps\n",
        "   - **Can be run anytime after Section 11**\n",
        "\n",
        "3. **Section 13**: t-SNE Visualization\n",
        "   - Loads pre-trained model\n",
        "   - Extracts test features (with corruption correction)\n",
        "   - Generates 2D t-SNE embeddings\n",
        "   - **Can be run anytime after Section 11**\n",
        "\n",
        "### ðŸ”§ Corruption Handling Feature\n",
        "\n",
        "The pipeline automatically handles corrupted test images:\n",
        "\n",
        "| Component | Description |\n",
        "|-----------|-------------|\n",
        "| `CorruptionHandlerFast` | Main class for detection and correction |\n",
        "| `load_clip_vectorized_test()` | Test-specific loading with corruption fix |\n",
        "| `extract_test_features_vectorized()` | Applies correction during feature extraction |\n",
        "\n",
        "**Statistics Tracking**: After test feature extraction, the pipeline prints:\n",
        "- Total frames processed\n",
        "- Number/percentage of inverted images fixed\n",
        "- Number/percentage of noisy images fixed\n",
        "\n",
        "### Key Features of Independent Cells\n",
        "\n",
        "âœ… **Each visualization cell is self-contained**\n",
        "- Loads necessary artifacts from disk\n",
        "- Checks for required files before execution\n",
        "- Provides clear error messages if dependencies missing\n",
        "\n",
        "âœ… **Can be run in any order** (after initial training)\n",
        "- Run heatmaps, then t-SNE, then heatmaps again\n",
        "- No need to re-train or re-extract features\n",
        "- Saves computation time during development\n",
        "\n",
        "âœ… **Saved Artifacts**\n",
        "- `best_model.pth`: Trained autoencoder weights\n",
        "- `train_stats.pth`: Training normalization statistics\n",
        "- `avenue_scores.csv`: Frame-level anomaly scores\n",
        "- `training_loss.png`: Training curve\n",
        "- `anomaly_analysis.png`: Score distribution\n",
        "- `pixel_heatmaps.png`: Grad-CAM heatmaps\n",
        "- `pixel_heatmaps_overlay.png`: Overlay version\n",
        "- `tsne_visualization.png`: t-SNE embedding\n",
        "\n",
        "### Usage Workflow\n",
        "\n",
        "```python\n",
        "# 1. First time: Run Section 11 (Training + Evaluation)\n",
        "# This trains and evaluates, saves all artifacts\n",
        "# Corruption correction is automatically applied to test data\n",
        "\n",
        "# 2. Generate visualizations independently:\n",
        "generate_heatmaps_independent()    # Anytime after training\n",
        "generate_tsne_independent()         # Anytime after training\n",
        "\n",
        "# 3. Can re-run visualizations without retraining:\n",
        "generate_heatmaps_independent()    # Runs in seconds\n",
        "generate_tsne_independent()         # Runs in minutes\n",
        "\n",
        "# 4. Modify settings and re-run:\n",
        "HEATMAP_VIDEO_IDX = 2             # Change which video to visualize\n",
        "HEATMAP_FRAMES = 16                # Change number of frames\n",
        "generate_heatmaps_independent()    # New heatmaps, no retraining\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
