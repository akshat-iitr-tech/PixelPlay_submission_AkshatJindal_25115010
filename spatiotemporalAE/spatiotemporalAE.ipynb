{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Cell 1: Imports, Config, Device\n",
    "# ===============================\n",
    "\n",
    "# ---- Standard libraries ----\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "# ---- Numerical & plotting ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Image processing ----\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# ---- PyTorch ----\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ---- Device setup (Kaggle P100) ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ---- Global project configuration ----\n",
    "IMG_SIZE = 227          # As used in the paper\n",
    "CHANNELS = 1            # Grayscale\n",
    "SEQ_LEN = 10            # Sequence length\n",
    "TEMPORAL_STRIDES = [1, 2, 3]  # Temporal data augmentation\n",
    "BATCH_SIZE = 4          # Safe for ConvLSTM on P100\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# ---- Dataset paths (Kaggle default) ----\n",
    "DATA_ROOT = \"/kaggle/input/avenue-dataset\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"training_videos\")\n",
    "TEST_DIR = os.path.join(DATA_ROOT, \"testing_videos\")\n",
    "\n",
    "# ---- Sanity print ----\n",
    "print(\"Train dir:\", TRAIN_DIR)\n",
    "print(\"Test dir :\", TEST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166473668af002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Cell 2: Dataset Sanity Checks\n",
    "# ===============================\n",
    "\n",
    "def list_video_folders(root_dir):\n",
    "    \"\"\"Return sorted list of video folders\"\"\"\n",
    "    if not os.path.exists(root_dir):\n",
    "        raise FileNotFoundError(f\"Directory not found: {root_dir}\")\n",
    "    return sorted([\n",
    "        d for d in os.listdir(root_dir)\n",
    "        if os.path.isdir(os.path.join(root_dir, d))\n",
    "    ])\n",
    "\n",
    "def count_frames(video_dir):\n",
    "    \"\"\"Count number of frame images in a video folder\"\"\"\n",
    "    frames = glob(os.path.join(video_dir, \"*.jpg\"))\n",
    "    return len(frames)\n",
    "\n",
    "# ---- List training and testing videos ----\n",
    "train_videos = list_video_folders(TRAIN_DIR)\n",
    "test_videos = list_video_folders(TEST_DIR)\n",
    "\n",
    "print(f\"Number of training videos: {len(train_videos)}\")\n",
    "print(f\"Number of testing videos : {len(test_videos)}\")\n",
    "\n",
    "# ---- Inspect a few videos ----\n",
    "print(\"\\nSample training videos and frame counts:\")\n",
    "for vid in train_videos[:3]:\n",
    "    vid_path = os.path.join(TRAIN_DIR, vid)\n",
    "    print(f\"  Video {vid}: {count_frames(vid_path)} frames\")\n",
    "\n",
    "print(\"\\nSample testing videos and frame counts:\")\n",
    "for vid in test_videos[:3]:\n",
    "    vid_path = os.path.join(TEST_DIR, vid)\n",
    "    print(f\"  Video {vid}: {count_frames(vid_path)} frames\")\n",
    "\n",
    "# ---- Basic consistency checks ----\n",
    "assert len(train_videos) > 0, \"No training videos found!\"\n",
    "assert len(test_videos) > 0, \"No testing videos found!\"\n",
    "\n",
    "# ---- Peek at frame naming ----\n",
    "sample_vid = train_videos[0]\n",
    "sample_frames = sorted(glob(os.path.join(TRAIN_DIR, sample_vid, \"*.jpg\")))\n",
    "\n",
    "print(\"\\nFirst 5 frame files in training video\", sample_vid)\n",
    "for f in sample_frames[:5]:\n",
    "    print(\" \", os.path.basename(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2be0a6969fc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# Cell 3: Image Preprocessing Utilities\n",
    "# =====================================\n",
    "\n",
    "def load_and_preprocess_image(\n",
    "    img_path,\n",
    "    mean_image=None,\n",
    "    std_image=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Load image, resize, grayscale, scale to [0,1],\n",
    "    subtract global mean image, normalize to zero mean & unit variance.\n",
    "    \"\"\"\n",
    "    # ---- Load image ----\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to read image: {img_path}\")\n",
    "\n",
    "    # ---- Resize ----\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # ---- Convert to grayscale ----\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ---- Scale to [0, 1] ----\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    # ---- Subtract global mean image ----\n",
    "    if mean_image is not None:\n",
    "        img = img - mean_image\n",
    "\n",
    "    # ---- Normalize to zero mean & unit variance ----\n",
    "    if std_image is not None:\n",
    "        img = img / (std_image + 1e-8)\n",
    "\n",
    "    # ---- Add channel dimension ----\n",
    "    img = np.expand_dims(img, axis=0)  # (1, H, W)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def compute_global_mean_std(train_dir, max_frames_per_video=None):\n",
    "    \"\"\"\n",
    "    Compute global mean image and std image\n",
    "    over the entire training dataset.\n",
    "\n",
    "    To keep it memory-safe, images are accumulated incrementally.\n",
    "    \"\"\"\n",
    "    print(\"Computing global mean and std from training data...\")\n",
    "\n",
    "    sum_image = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float64)\n",
    "    sum_sq_image = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float64)\n",
    "    total_frames = 0\n",
    "\n",
    "    video_folders = sorted(os.listdir(train_dir))\n",
    "\n",
    "    for vid in video_folders:\n",
    "        vid_path = os.path.join(train_dir, vid)\n",
    "        frame_paths = sorted(glob(os.path.join(vid_path, \"*.jpg\")))\n",
    "\n",
    "        if max_frames_per_video is not None:\n",
    "            frame_paths = frame_paths[:max_frames_per_video]\n",
    "\n",
    "        for fp in frame_paths:\n",
    "            img = cv2.imread(fp)\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "\n",
    "            sum_image += img\n",
    "            sum_sq_image += img ** 2\n",
    "            total_frames += 1\n",
    "\n",
    "    mean_image = sum_image / total_frames\n",
    "    var_image = (sum_sq_image / total_frames) - (mean_image ** 2)\n",
    "    std_image = np.sqrt(np.maximum(var_image, 1e-8))\n",
    "\n",
    "    print(f\"Computed mean/std using {total_frames} frames\")\n",
    "\n",
    "    return mean_image.astype(np.float32), std_image.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e39d2e9c8a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 4: Temporal Sequence Index Generation\n",
    "# ==========================================\n",
    "\n",
    "def generate_sequences_for_video(\n",
    "    frame_paths,\n",
    "    seq_len=SEQ_LEN,\n",
    "    strides=TEMPORAL_STRIDES\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate temporal sequences (lists of frame indices)\n",
    "    for a single video using multiple temporal strides.\n",
    "\n",
    "    Returns:\n",
    "        List of lists, where each sublist contains frame indices.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    num_frames = len(frame_paths)\n",
    "\n",
    "    for stride in strides:\n",
    "        max_start = num_frames - (seq_len - 1) * stride\n",
    "        for start_idx in range(max_start):\n",
    "            seq_indices = [\n",
    "                start_idx + i * stride\n",
    "                for i in range(seq_len)\n",
    "            ]\n",
    "            sequences.append(seq_indices)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def build_sequence_index(root_dir):\n",
    "    \"\"\"\n",
    "    Build sequence index for all videos in a directory.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with:\n",
    "        {\n",
    "            'video': video_name,\n",
    "            'frame_indices': [i1, i2, ..., iT]\n",
    "        }\n",
    "    \"\"\"\n",
    "    all_sequences = []\n",
    "\n",
    "    video_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "    for vid in video_folders:\n",
    "        vid_path = os.path.join(root_dir, vid)\n",
    "        frame_paths = sorted(glob(os.path.join(vid_path, \"*.jpg\")))\n",
    "\n",
    "        if len(frame_paths) < SEQ_LEN:\n",
    "            continue\n",
    "\n",
    "        seqs = generate_sequences_for_video(\n",
    "            frame_paths,\n",
    "            seq_len=SEQ_LEN,\n",
    "            strides=TEMPORAL_STRIDES\n",
    "        )\n",
    "\n",
    "        for seq in seqs:\n",
    "            all_sequences.append({\n",
    "                \"video\": vid,\n",
    "                \"frame_indices\": seq\n",
    "            })\n",
    "\n",
    "    return all_sequences\n",
    "\n",
    "\n",
    "# ---- Build training sequence index ----\n",
    "train_sequences = build_sequence_index(TRAIN_DIR)\n",
    "\n",
    "print(f\"Total training sequences generated: {len(train_sequences)}\")\n",
    "\n",
    "# ---- Inspect a few sequences ----\n",
    "for i in range(3):\n",
    "    print(train_sequences[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe8d13279e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 5: Spatial Encoder\n",
    "# ==========================================\n",
    "\n",
    "class SpatialEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=CHANNELS,\n",
    "                out_channels=128,\n",
    "                kernel_size=11,\n",
    "                stride=4,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(\n",
    "                in_channels=128,\n",
    "                out_channels=64,\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        return self.encoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa8acabf9d7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 6A: ConvLSTM Cell\n",
    "# ==========================================\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_dim + hidden_dim,\n",
    "            out_channels=4 * hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        combined = torch.cat([x, h_prev], dim=1)\n",
    "        conv_out = self.conv(combined)\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.chunk(conv_out, 4, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c = f * c_prev + i * g\n",
    "        h = o * torch.tanh(c)\n",
    "\n",
    "        return h, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a54d9f1fe8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 6B: ConvLSTM Stack\n",
    "# ==========================================\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.cell1 = ConvLSTMCell(64, 64, 3)\n",
    "        self.cell2 = ConvLSTMCell(64, 32, 3)\n",
    "        self.cell3 = ConvLSTMCell(32, 64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, C, H, W)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.size()\n",
    "\n",
    "        h1 = torch.zeros(B, 64, H, W, device=x.device)\n",
    "        c1 = torch.zeros_like(h1)\n",
    "\n",
    "        h2 = torch.zeros(B, 32, H, W, device=x.device)\n",
    "        c2 = torch.zeros_like(h2)\n",
    "\n",
    "        h3 = torch.zeros(B, 64, H, W, device=x.device)\n",
    "        c3 = torch.zeros_like(h3)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(T):\n",
    "            h1, c1 = self.cell1(x[:, t], h1, c1)\n",
    "            h2, c2 = self.cell2(h1, h2, c2)\n",
    "            h3, c3 = self.cell3(h2, h3, c3)\n",
    "            outputs.append(h3.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f4d39ad1aeec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 7: Spatial Decoder\n",
    "# ==========================================\n",
    "\n",
    "class SpatialDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialDecoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=128,\n",
    "                out_channels=CHANNELS,\n",
    "                kernel_size=11,\n",
    "                stride=4,\n",
    "                padding=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        return self.decoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61263c2bd110d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 8: Full Spatiotemporal Autoencoder\n",
    "# ==========================================\n",
    "\n",
    "class SpatioTemporalAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatioTemporalAutoEncoder, self).__init__()\n",
    "\n",
    "        self.spatial_encoder = SpatialEncoder()\n",
    "        self.temporal_model = ConvLSTM()\n",
    "        self.spatial_decoder = SpatialDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, C, H, W)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.size()\n",
    "\n",
    "        # ---- Encode each frame spatially ----\n",
    "        encoded = []\n",
    "        for t in range(T):\n",
    "            feat = self.spatial_encoder(x[:, t])\n",
    "            encoded.append(feat.unsqueeze(1))\n",
    "\n",
    "        encoded = torch.cat(encoded, dim=1)  # (B, T, 64, 26, 26)\n",
    "\n",
    "        # ---- Temporal encoding/decoding ----\n",
    "        temporal_out = self.temporal_model(encoded)\n",
    "\n",
    "        # ---- Decode each frame spatially ----\n",
    "        decoded = []\n",
    "        for t in range(T):\n",
    "            frame = self.spatial_decoder(temporal_out[:, t])\n",
    "            decoded.append(frame.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(decoded, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18885bb79fc095f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 9: Model Initialization & Params\n",
    "# ==========================================\n",
    "\n",
    "model = SpatioTemporalAutoEncoder().to(device)\n",
    "\n",
    "# ---- Parameter count ----\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = count_parameters(model)\n",
    "\n",
    "print(\"Model initialized.\")\n",
    "print(f\"Total trainable parameters: {num_params / 1e6:.2f} M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16269eb396c02a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 10: Loss Function & Optimizer\n",
    "# ==========================================\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "print(\"Loss and optimizer initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a9294a3598fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 11: One Training Epoch\n",
    "# ==========================================\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # batch: (B, T, C, H, W)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ---- Forward ----\n",
    "        recon = model(batch)\n",
    "\n",
    "        # ---- Reconstruction loss ----\n",
    "        loss = criterion(recon, batch)\n",
    "\n",
    "        # ---- Backprop ----\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(dataloader)} - Loss: {loss.item():.6f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 12: Train Dataset & DataLoader Setup\n",
    "# ==========================================\n",
    "\n",
    "# ---- Compute global mean and std ----\n",
    "mean_image, std_image = compute_global_mean_std(TRAIN_DIR)\n",
    "\n",
    "print(f\"Global mean image shape: {mean_image.shape}\")\n",
    "print(f\"Global std image shape: {std_image.shape}\")\n",
    "print(f\"Mean value range: [{mean_image.min():.3f}, {mean_image.max():.3f}]\")\n",
    "print(f\"Std value range: [{std_image.min():.3f}, {std_image.max():.3f}]\")\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \"\"\"Dataset for training sequences\"\"\"\n",
    "    def __init__(self, root_dir, mean_img, std_img):\n",
    "        self.samples = []\n",
    "        self.root_dir = root_dir\n",
    "        self.mean_img = mean_img\n",
    "        self.std_img = std_img\n",
    "\n",
    "        videos = sorted(os.listdir(root_dir))\n",
    "\n",
    "        for vid in videos:\n",
    "            frame_paths = sorted(glob(os.path.join(root_dir, vid, \"*.jpg\")))\n",
    "            \n",
    "            if len(frame_paths) < SEQ_LEN:\n",
    "                continue\n",
    "            \n",
    "            seqs = generate_sequences_for_video(frame_paths)\n",
    "\n",
    "            for seq in seqs:\n",
    "                self.samples.append((frame_paths, seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, seq = self.samples[idx]\n",
    "\n",
    "        frames = []\n",
    "        for i in seq:\n",
    "            img = load_and_preprocess_image(\n",
    "                frame_paths[i],\n",
    "                self.mean_img,\n",
    "                self.std_img\n",
    "            )\n",
    "            frames.append(img)\n",
    "\n",
    "        # Shape: (T, C, H, W)\n",
    "        frames = torch.tensor(np.stack(frames), dtype=torch.float32)\n",
    "        return frames\n",
    "\n",
    "\n",
    "# ---- Create training dataset ----\n",
    "train_dataset = TrainDataset(TRAIN_DIR, mean_image, std_image)\n",
    "\n",
    "# ---- Create training dataloader ----\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining dataset size: {len(train_dataset)}\")\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0105f2502d04ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 12: Full Training Loop\n",
    "# ==========================================\n",
    "\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    epoch_loss = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}] - Loss: {epoch_loss:.6f} - Time: {elapsed:.2f}s\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, f\"model_epoch_{epoch}.pth\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Checkpoint saved: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae58ccc54f7447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 13: Test Dataset\n",
    "# ==========================================\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_dir, mean_img, std_img):\n",
    "        self.samples = []\n",
    "        self.root_dir = root_dir\n",
    "        self.mean_img = mean_img\n",
    "        self.std_img = std_img\n",
    "\n",
    "        videos = sorted(os.listdir(root_dir))\n",
    "\n",
    "        for vid in videos:\n",
    "            frame_paths = sorted(glob(os.path.join(root_dir, vid, \"*.jpg\")))\n",
    "            seqs = generate_sequences_for_video(frame_paths)\n",
    "\n",
    "            for seq in seqs:\n",
    "                self.samples.append((vid, frame_paths, seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid, frame_paths, seq = self.samples[idx]\n",
    "\n",
    "        frames = []\n",
    "        for i in seq:\n",
    "            img = load_and_preprocess_image(\n",
    "                frame_paths[i],\n",
    "                self.mean_img,\n",
    "                self.std_img\n",
    "            )\n",
    "            frames.append(img)\n",
    "\n",
    "        frames = torch.tensor(np.stack(frames), dtype=torch.float32)\n",
    "        return frames, vid, seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5df9c55d63980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 14: Inference & Euclidean Error\n",
    "# ==========================================\n",
    "\n",
    "model.eval()\n",
    "frame_errors = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, vids, seqs in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        recon = model(batch)\n",
    "\n",
    "        # Euclidean distance per frame\n",
    "        error = torch.sqrt(\n",
    "            torch.sum((recon - batch) ** 2, dim=(2, 3, 4))\n",
    "        )  # (B, T)\n",
    "\n",
    "        error = error.cpu().numpy()\n",
    "\n",
    "        for b in range(batch.size(0)):\n",
    "            vid = vids[b]\n",
    "            seq = seqs[b]\n",
    "\n",
    "            if vid not in frame_errors:\n",
    "                frame_errors[vid] = {}\n",
    "\n",
    "            for t, frame_idx in enumerate(seq):\n",
    "                frame_errors[vid].setdefault(frame_idx, []).append(error[b, t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c035ac484859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 15: Aggregate Frame Scores\n",
    "# ==========================================\n",
    "\n",
    "frame_scores = {}\n",
    "\n",
    "for vid in frame_errors:\n",
    "    frame_scores[vid] = {}\n",
    "    for frame_idx, errs in frame_errors[vid].items():\n",
    "        frame_scores[vid][frame_idx] = np.mean(errs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e9a11dc4cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 16: Regularity Score\n",
    "# ==========================================\n",
    "\n",
    "all_scores = []\n",
    "for vid in frame_scores:\n",
    "    all_scores.extend(frame_scores[vid].values())\n",
    "\n",
    "min_s, max_s = min(all_scores), max(all_scores)\n",
    "\n",
    "regularity_scores = {}\n",
    "\n",
    "for vid in frame_scores:\n",
    "    regularity_scores[vid] = {}\n",
    "    for frame_idx, score in frame_scores[vid].items():\n",
    "        norm_score = (score - min_s) / (max_s - min_s + 1e-8)\n",
    "        regularity_scores[vid][frame_idx] = 1.0 - norm_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beac8d4050e200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 17: Persistence1D Event Detection\n",
    "# ==========================================\n",
    "\n",
    "TEMPORAL_WINDOW = 50  # frames\n",
    "PERSISTENCE_THRESHOLD = 0.1  # Minimum persistence value to consider as anomaly\n",
    "\n",
    "def persistence_1d(regularity_scores_dict, temporal_window=TEMPORAL_WINDOW):\n",
    "    \"\"\"\n",
    "    Persistence1D algorithm for grouping local minima.\n",
    "    \n",
    "    Low regularity scores (< 0.5) indicate potential anomalies.\n",
    "    We find connected components of frames with low scores\n",
    "    within the temporal window, then compute persistence.\n",
    "    \n",
    "    Args:\n",
    "        regularity_scores_dict: {frame_idx: regularity_score}\n",
    "        temporal_window: Max frames apart to consider connected\n",
    "    \n",
    "    Returns:\n",
    "        events: List of {\n",
    "            'frames': [frame indices],\n",
    "            'min_score': minimum score in group,\n",
    "            'persistence': max persistence value,\n",
    "            'mean_score': average score\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---- Find potential anomaly frames (low regularity) ----\n",
    "    frames = sorted(regularity_scores_dict.keys())\n",
    "    \n",
    "    # Create a mapping of frame -> score\n",
    "    score_map = {f: regularity_scores_dict[f] for f in frames}\n",
    "    \n",
    "    # ---- Build connected components via union-find ----\n",
    "    # Frames with low regularity that are close in time belong together\n",
    "    parent = {f: f for f in frames}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    # Connect frames within temporal window\n",
    "    for i in range(len(frames) - 1):\n",
    "        if frames[i + 1] - frames[i] <= temporal_window:\n",
    "            union(frames[i], frames[i + 1])\n",
    "    \n",
    "    # ---- Group frames by component ----\n",
    "    components = {}\n",
    "    for f in frames:\n",
    "        root = find(f)\n",
    "        if root not in components:\n",
    "            components[root] = []\n",
    "        components[root].append(f)\n",
    "    \n",
    "    # ---- Compute persistence for each component ----\n",
    "    events = []\n",
    "    \n",
    "    for root, component_frames in components.items():\n",
    "        component_frames = sorted(component_frames)\n",
    "        \n",
    "        # Persistence: measure of how strong/persistent the anomaly is\n",
    "        # Higher persistence = more consistent anomaly\n",
    "        scores = [score_map[f] for f in component_frames]\n",
    "        \n",
    "        # Persistence metric: lower regularity = higher persistence\n",
    "        min_score = min(scores)\n",
    "        mean_score = np.mean(scores)\n",
    "        persistence = 1.0 - mean_score  # Invert: low regularity â†’ high persistence\n",
    "        \n",
    "        # ---- Span of the event ----\n",
    "        time_span = component_frames[-1] - component_frames[0]\n",
    "        \n",
    "        events.append({\n",
    "            'frames': component_frames,\n",
    "            'min_score': min_score,\n",
    "            'mean_score': mean_score,\n",
    "            'persistence': persistence,\n",
    "            'time_span': time_span,\n",
    "            'num_frames': len(component_frames)\n",
    "        })\n",
    "    \n",
    "    # Sort by persistence (strongest anomalies first)\n",
    "    events.sort(key=lambda e: e['persistence'], reverse=True)\n",
    "    \n",
    "    return events\n",
    "\n",
    "\n",
    "def filter_events_by_persistence(events, threshold=PERSISTENCE_THRESHOLD):\n",
    "    \"\"\"Filter events by persistence threshold.\"\"\"\n",
    "    return [e for e in events if e['persistence'] >= threshold]\n",
    "\n",
    "\n",
    "# ---- Apply Persistence1D to each video ----\n",
    "anomalous_events = {}\n",
    "\n",
    "for vid in regularity_scores:\n",
    "    events = persistence_1d(regularity_scores[vid], TEMPORAL_WINDOW)\n",
    "    \n",
    "    # Filter by persistence threshold\n",
    "    significant_events = filter_events_by_persistence(events, PERSISTENCE_THRESHOLD)\n",
    "    \n",
    "    anomalous_events[vid] = significant_events\n",
    "    \n",
    "    print(f\"\\n{vid}:\")\n",
    "    print(f\"  Total events detected: {len(events)}\")\n",
    "    print(f\"  Significant events (persistence >= {PERSISTENCE_THRESHOLD}): {len(significant_events)}\")\n",
    "    \n",
    "    for idx, event in enumerate(significant_events[:3]):  # Show top 3\n",
    "        print(f\"    Event {idx+1}: frames {event['frames'][0]}-{event['frames'][-1]}, \"\n",
    "              f\"persistence={event['persistence']:.3f}, \"\n",
    "              f\"mean_regularity={event['mean_score']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592eb089b26601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 18: CSV Output with Persistence1D Results\n",
    "# ==========================================\n",
    "\n",
    "# ---- Save frame-level anomaly scores ----\n",
    "rows = []\n",
    "\n",
    "for vid in sorted(regularity_scores):\n",
    "    for frame_idx in sorted(regularity_scores[vid]):\n",
    "        rows.append({\n",
    "            \"Id\": f\"{vid}_{frame_idx}\",\n",
    "            \"Score\": regularity_scores[vid][frame_idx]\n",
    "        })\n",
    "\n",
    "df_frames = pd.DataFrame(rows)\n",
    "df_frames.to_csv(\"anomaly_scores.csv\", index=False)\n",
    "\n",
    "print(\"Frame-level scores saved as anomaly_scores.csv\")\n",
    "print(df_frames.head())\n",
    "\n",
    "# ---- Save event-level detections ----\n",
    "event_rows = []\n",
    "event_id = 1\n",
    "\n",
    "for vid in sorted(anomalous_events):\n",
    "    for event in anomalous_events[vid]:\n",
    "        event_rows.append({\n",
    "            \"Event_ID\": event_id,\n",
    "            \"Video\": vid,\n",
    "            \"Start_Frame\": event['frames'][0],\n",
    "            \"End_Frame\": event['frames'][-1],\n",
    "            \"Num_Frames\": event['num_frames'],\n",
    "            \"Time_Span_Frames\": event['time_span'],\n",
    "            \"Persistence\": event['persistence'],\n",
    "            \"Mean_Regularity\": event['mean_score'],\n",
    "            \"Min_Regularity\": event['min_score']\n",
    "        })\n",
    "        event_id += 1\n",
    "\n",
    "df_events = pd.DataFrame(event_rows)\n",
    "df_events.to_csv(\"detected_events.csv\", index=False)\n",
    "\n",
    "print(\"\\n\\nEvent-level detections saved as detected_events.csv\")\n",
    "print(df_events.head(10))\n",
    "print(f\"\\nTotal anomalous events detected: {len(df_events)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592cff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 19: Visualization of Persistence1D Results\n",
    "# ==========================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize results for a few videos\n",
    "n_videos_to_plot = min(3, len(anomalous_events))\n",
    "\n",
    "for vid_idx, vid in enumerate(sorted(anomalous_events.keys())[:n_videos_to_plot]):\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    \n",
    "    # Plot regularity scores\n",
    "    frames = sorted(regularity_scores[vid].keys())\n",
    "    scores = [regularity_scores[vid][f] for f in frames]\n",
    "    \n",
    "    ax.plot(frames, scores, 'b-', linewidth=1, label='Regularity Score', alpha=0.7)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Anomaly Threshold')\n",
    "    \n",
    "    # Highlight detected events\n",
    "    events = anomalous_events[vid]\n",
    "    colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(events)))\n",
    "    \n",
    "    for event_idx, event in enumerate(events):\n",
    "        event_frames = event['frames']\n",
    "        if event_frames:\n",
    "            ax.axvspan(event_frames[0], event_frames[-1], \n",
    "                      alpha=0.2, color=colors[event_idx],\n",
    "                      label=f\"Event {event_idx+1} (persist={event['persistence']:.2f})\")\n",
    "    \n",
    "    ax.set_xlabel('Frame Index', fontsize=11)\n",
    "    ax.set_ylabel('Regularity Score (1=normal, 0=anomaly)', fontsize=11)\n",
    "    ax.set_title(f'{vid} - Persistence1D Anomaly Detection\\n'\n",
    "                f'Window={TEMPORAL_WINDOW}f, Threshold={PERSISTENCE_THRESHOLD}',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"persistence1d_{vid}.png\", dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved plot: persistence1d_{vid}.png\")\n",
    "\n",
    "# ---- Summary statistics ----\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERSISTENCE1D SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_events = sum(len(anomalous_events[vid]) for vid in anomalous_events)\n",
    "print(f\"Total videos analyzed: {len(anomalous_events)}\")\n",
    "print(f\"Total anomalous events detected: {total_events}\")\n",
    "\n",
    "all_persistences = []\n",
    "all_spans = []\n",
    "\n",
    "for vid in anomalous_events:\n",
    "    for event in anomalous_events[vid]:\n",
    "        all_persistences.append(event['persistence'])\n",
    "        all_spans.append(event['time_span'])\n",
    "\n",
    "if all_persistences:\n",
    "    print(f\"\\nPersistence statistics:\")\n",
    "    print(f\"  Mean: {np.mean(all_persistences):.3f}\")\n",
    "    print(f\"  Median: {np.median(all_persistences):.3f}\")\n",
    "    print(f\"  Min: {np.min(all_persistences):.3f}\")\n",
    "    print(f\"  Max: {np.max(all_persistences):.3f}\")\n",
    "    \n",
    "    print(f\"\\nEvent duration (frames):\")\n",
    "    print(f\"  Mean: {np.mean(all_spans):.1f}\")\n",
    "    print(f\"  Median: {np.median(all_spans):.1f}\")\n",
    "    print(f\"  Min: {np.min(all_spans):.0f}\")\n",
    "    print(f\"  Max: {np.max(all_spans):.0f}\")\n",
    "    print(f\"  (~2-3 sec at 24-25 fps = 48-75 frames)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
