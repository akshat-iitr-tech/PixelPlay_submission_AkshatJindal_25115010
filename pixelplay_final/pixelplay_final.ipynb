{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Video Anomaly Detection using Multi-Layer Reconstruction Autoencoder\n",
        "\n",
        "### Key Features\n",
        "- **3D Convolutional Autoencoder** for spatiotemporal feature learning\n",
        "- **Variance Attention Mechanism** to focus on important regions\n",
        "- **Multi-Layer Reconstruction** for improved anomaly detection\n",
        "- **PyTorch Lightning** for efficient training\n",
        "- **Early Stopping & Checkpointing** for optimal results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1Ô∏è‚É£ Import Libraries\n",
        "\n",
        "Import all necessary dependencies for deep learning, data processing, and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch and PyTorch Lightning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "print(\"‚úì All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2Ô∏è‚É£ Configuration\n",
        "\n",
        "### üìÅ Dataset Paths\n",
        "Update these paths according to your Kaggle dataset location.\n",
        "\n",
        "### ‚öôÔ∏è Hyperparameters\n",
        "- **Image Size**: resized to 448w x 256h to maintain aspect ratio\n",
        "- **Sequence Length**: 8 frames per clip\n",
        "- **Batch Size**: 4\n",
        "- **Max Epochs**: 80 (with early stopping)\n",
        "- **Learning Rate**: 0.0001\n",
        "- **Early Stopping Patience**: 8 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # ==================== DATASET PATHS ====================\n",
        "    # Update these paths for your environment\n",
        "    BASE_PATH = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset'\n",
        "    TRAIN_PATH = f'{BASE_PATH}/training_videos'  # Training .jpg frames\n",
        "    TEST_PATH = f'{BASE_PATH}/testing_videos'    # Testing .jpg frames\n",
        "    \n",
        "    # ==================== MODEL PARAMETERS ====================\n",
        "    IMG_HEIGHT = 256  # Padded from 360 to be divisible by 16\n",
        "    IMG_WIDTH = 448\n",
        "    SEQUENCE_LENGTH = 8  # Number of frames per sequence\n",
        "    CHANNELS = 3         # RGB channels\n",
        "    \n",
        "    # ==================== TRAINING PARAMETERS ====================\n",
        "    BATCH_SIZE = 4\n",
        "    MAX_EPOCHS = 80      \n",
        "    LEARNING_RATE = 0.0001\n",
        "    WEIGHT_DECAY = 0.0001\n",
        "    PATIENCE = 6         # Early stopping patience\n",
        "    \n",
        "    # ==================== LOSS WEIGHTS ====================\n",
        "    ALPHA_1 = 1.0   # patAppearance loss weight\n",
        "    ALPHA_2 = 25.0  # Motion loss weight (higher = more focus on motion)\n",
        "    \n",
        "    # ==================== SCORE WEIGHTS ====================\n",
        "    # These are tuned for the Avenue dataset\n",
        "    BETA_1 = -0.1   # Appearance score weight\n",
        "    BETA_2 = 2.0    # Motion score weight\n",
        "    \n",
        "    # ==================== MULTI-LAYER RECONSTRUCTION ====================\n",
        "    RECON_LAYERS = [0, 2]  \n",
        "    \n",
        "    # ==================== OUTPUT ====================\n",
        "    OUTPUT_CSV = 'anomaly_scores.csv'\n",
        "    CHECKPOINT_DIR = 'checkpoints'\n",
        "\n",
        "# Initialize config\n",
        "config = Config()\n",
        "print(\"‚úì Configuration loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3Ô∏è‚É£ Early Dataset Validation\n",
        "\n",
        "Before training, let's verify that our dataset is correctly loaded by counting frames in both training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define and run dataset validation function\n",
        "def get_dataset_frame_counts(train_path, test_path):\n",
        "    \"\"\"\n",
        "    Retrieve the number of frames in train and test datasets.\n",
        "    This validates data availability before training begins.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EARLY DATASET VALIDATION - Counting Frames\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    results = {\n",
        "        'train': {'videos': 0, 'total_frames': 0, 'frames_per_video': []},\n",
        "        'test': {'videos': 0, 'total_frames': 0, 'frames_per_video': []}\n",
        "    }\n",
        "    \n",
        "    # Count training frames\n",
        "    if os.path.exists(train_path):\n",
        "        train_videos = sorted([d for d in os.listdir(train_path) \n",
        "                              if os.path.isdir(os.path.join(train_path, d))])\n",
        "        results['train']['videos'] = len(train_videos)\n",
        "        \n",
        "        for video_folder in train_videos:\n",
        "            video_path = os.path.join(train_path, video_folder)\n",
        "            frames = glob.glob(os.path.join(video_path, '*.jpg'))\n",
        "            frame_count = len(frames)\n",
        "            results['train']['frames_per_video'].append(frame_count)\n",
        "            results['train']['total_frames'] += frame_count\n",
        "        \n",
        "        print(f\"\\nüìä TRAINING DATASET:\")\n",
        "        print(f\"   - Number of videos: {results['train']['videos']}\")\n",
        "        print(f\"   - Total frames: {results['train']['total_frames']}\")\n",
        "        print(f\"   - Average frames per video: {np.mean(results['train']['frames_per_video']):.1f}\")\n",
        "        print(f\"   - Min frames: {min(results['train']['frames_per_video']) if results['train']['frames_per_video'] else 0}\")\n",
        "        print(f\"   - Max frames: {max(results['train']['frames_per_video']) if results['train']['frames_per_video'] else 0}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: Training path not found: {train_path}\")\n",
        "    \n",
        "    # Count testing frames\n",
        "    if os.path.exists(test_path):\n",
        "        test_videos = sorted([d for d in os.listdir(test_path) \n",
        "                             if os.path.isdir(os.path.join(test_path, d))])\n",
        "        results['test']['videos'] = len(test_videos)\n",
        "        \n",
        "        for video_folder in test_videos:\n",
        "            video_path = os.path.join(test_path, video_folder)\n",
        "            frames = glob.glob(os.path.join(video_path, '*.jpg'))\n",
        "            frame_count = len(frames)\n",
        "            results['test']['frames_per_video'].append(frame_count)\n",
        "            results['test']['total_frames'] += frame_count\n",
        "        \n",
        "        print(f\"\\nüìä TESTING DATASET:\")\n",
        "        print(f\"   - Number of videos: {results['test']['videos']}\")\n",
        "        print(f\"   - Total frames: {results['test']['total_frames']}\")\n",
        "        print(f\"   - Average frames per video: {np.mean(results['test']['frames_per_video']):.1f}\")\n",
        "        print(f\"   - Min frames: {min(results['test']['frames_per_video']) if results['test']['frames_per_video'] else 0}\")\n",
        "        print(f\"   - Max frames: {max(results['test']['frames_per_video']) if results['test']['frames_per_video'] else 0}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: Testing path not found: {test_path}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úì Frame count validation completed\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run validation immediately\n",
        "frame_counts = get_dataset_frame_counts(config.TRAIN_PATH, config.TEST_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4Ô∏è‚É£ Dataset Class\n",
        "\n",
        "Custom PyTorch Dataset for loading video frame sequences. \n",
        "\n",
        "### üîë Key Features:\n",
        "- Loads `.jpg` frames from video directories\n",
        "- Creates sequences of 8 consecutive frames\n",
        "- **Training mode**: Non-overlapping sequences (faster)\n",
        "- **Testing mode**: Sliding window with stride=1 (complete coverage)\n",
        "- Normalizes pixel values to [0, 1]\n",
        "- Returns tensors in shape: `(C, T, H, W)` = `(3, 8, 300, 528)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AvenueDataset(Dataset):\n",
        "    \"\"\"Dataset for Avenue video frames (.jpg files)\"\"\"\n",
        "    \n",
        "    def __init__(self, root_dir, sequence_length=8, mode='train', transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.sequence_length = sequence_length\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Get all video folders\n",
        "        self.video_folders = sorted([d for d in os.listdir(root_dir) \n",
        "                                     if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        \n",
        "        # Build frame sequences\n",
        "        self.sequences = []\n",
        "        self.video_ids = []\n",
        "        \n",
        "        for video_id, video_folder in enumerate(self.video_folders):\n",
        "            video_path = os.path.join(root_dir, video_folder)\n",
        "            frames = sorted(glob.glob(os.path.join(video_path, '*.jpg')))\n",
        "            \n",
        "            # Create sequences\n",
        "            # Train: non-overlapping (stride = sequence_length)\n",
        "            # Test: sliding window (stride = 1)\n",
        "            stride = sequence_length if mode == 'train' else 1\n",
        "            for i in range(0, len(frames) - sequence_length + 1, stride):\n",
        "                seq_frames = frames[i:i + sequence_length]\n",
        "                if len(seq_frames) == sequence_length:\n",
        "                    self.sequences.append(seq_frames)\n",
        "                    self.video_ids.append(video_id)\n",
        "        \n",
        "        print(f\"{mode.upper()} dataset: {len(self.sequences)} sequences from {len(self.video_folders)} videos\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        frame_paths = self.sequences[idx]\n",
        "        video_id = self.video_ids[idx]\n",
        "        \n",
        "        # Load frames\n",
        "        frames = []\n",
        "        for frame_path in frame_paths:\n",
        "            img = Image.open(frame_path).convert('RGB')\n",
        "            img = img.resize((config.IMG_WIDTH, config.IMG_HEIGHT))\n",
        "            img = np.array(img) / 255.0  # Normalize to [0, 1]\n",
        "            frames.append(img)\n",
        "        \n",
        "        # Stack frames: (T, H, W, C)\n",
        "        frames = np.stack(frames, axis=0)\n",
        "        \n",
        "        # Convert to torch tensor: (C, T, H, W)\n",
        "        frames = torch.from_numpy(frames).float().permute(3, 0, 1, 2)\n",
        "        \n",
        "        return {\n",
        "            'frames': frames,\n",
        "            'video_id': video_id,\n",
        "            'frame_idx': idx\n",
        "        }\n",
        "\n",
        "print(\"‚úì Dataset class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5Ô∏è‚É£ Model Architecture\n",
        "\n",
        "### üèóÔ∏è 3D Convolutional Autoencoder\n",
        "\n",
        "The model consists of:\n",
        "1. **Encoder**: 4 downsampling blocks (3‚Üí64‚Üí128‚Üí256‚Üí512 channels)\n",
        "2. **Decoder**: 4 upsampling blocks (512‚Üí256‚Üí128‚Üí64‚Üí3 channels)\n",
        "3. **Multi-layer outputs**: Stores intermediate features for reconstruction\n",
        "\n",
        "### üîç Why 3D Convolutions?\n",
        "- Captures **spatial** patterns (what's in the frame)\n",
        "- Captures **temporal** patterns (how things move)\n",
        "- Better than 2D for video understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DownsamplingBlock(nn.Module):\n",
        "    \"\"\"3D Downsampling block with spatial-temporal convolutions\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
        "        self.conv3 = nn.Conv3d(out_channels, out_channels, kernel_size=1, stride=2, padding=0)\n",
        "        self.bn3 = nn.BatchNorm3d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.relu(self.bn3(self.conv3(x)))  # Downsample by 2x\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsamplingBlock(nn.Module):\n",
        "    \"\"\"3D Upsampling block with spatial-temporal convolutions\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
        "        self.conv3 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm3d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.upsample(x)  # Upsample by 2x\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiLayerAutoencoder(nn.Module):\n",
        "    \"\"\"Multi-layer reconstruction autoencoder for video anomaly detection\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels=3):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Encoder: progressively downsample and increase channels\n",
        "        self.enc1 = DownsamplingBlock(in_channels, 64)\n",
        "        self.enc2 = DownsamplingBlock(64, 128)\n",
        "        self.enc3 = DownsamplingBlock(128, 256)\n",
        "        self.enc4 = DownsamplingBlock(256, 512)\n",
        "        \n",
        "        # Decoder: progressively upsample and decrease channels\n",
        "        self.dec4 = UpsamplingBlock(512, 256)\n",
        "        self.dec3 = UpsamplingBlock(256, 128)\n",
        "        self.dec2 = UpsamplingBlock(128, 64)\n",
        "        self.dec1 = UpsamplingBlock(64, in_channels)\n",
        "        \n",
        "        # Final output layer\n",
        "        self.final = nn.Conv3d(in_channels, in_channels, kernel_size=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Store intermediate features for multi-layer reconstruction\n",
        "        features = {'enc': [x], 'dec': []}\n",
        "        \n",
        "        # Encoder path\n",
        "        x1 = self.enc1(x)\n",
        "        features['enc'].append(x1)\n",
        "        \n",
        "        x2 = self.enc2(x1)\n",
        "        features['enc'].append(x2)\n",
        "        \n",
        "        x3 = self.enc3(x2)\n",
        "        features['enc'].append(x3)\n",
        "        \n",
        "        x4 = self.enc4(x3)\n",
        "        features['enc'].append(x4)\n",
        "        \n",
        "        # Decoder path\n",
        "        x = self.dec4(x4)\n",
        "        features['dec'].append(x)\n",
        "        \n",
        "        x = self.dec3(x)\n",
        "        features['dec'].append(x)\n",
        "        \n",
        "        x = self.dec2(x)\n",
        "        features['dec'].append(x)\n",
        "        \n",
        "        x = self.dec1(x)\n",
        "        features['dec'].append(x)\n",
        "        \n",
        "        # Final reconstruction\n",
        "        x = self.final(x)\n",
        "        features['dec'].insert(0, x)\n",
        "        \n",
        "        return x, features\n",
        "\n",
        "print(\"‚úì Model architecture defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6Ô∏è‚É£ Variance Attention Mechanism\n",
        "\n",
        "### üí° What is Variance Attention?\n",
        "\n",
        "Focuses the model on regions with high variance, which often indicate:\n",
        "- **Movement** (temporal variance)\n",
        "- **Complex patterns** (channel variance)\n",
        "- **Potential anomalies** (unusual variations)\n",
        "\n",
        "### üìê How it works:\n",
        "1. Compute variance across channels and time\n",
        "2. Apply softmax to create attention weights\n",
        "3. Multiply reconstruction loss by attention weights\n",
        "4. Model learns to pay more attention to important regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_variance_attention(feature_map):\n",
        "    \"\"\"\n",
        "    Compute variance attention along channel and temporal dimensions.\n",
        "    \n",
        "    Args:\n",
        "        feature_map: (B, C, T, H, W) - Batch, Channels, Time, Height, Width\n",
        "    \n",
        "    Returns:\n",
        "        channel_attention: (B, 1, T, H, W) - Attention across channels\n",
        "        temporal_attention: (B, C, 1, H, W) - Attention across time\n",
        "    \"\"\"\n",
        "    B, C, T, H, W = feature_map.shape\n",
        "    \n",
        "    # === Channel Variance Attention ===\n",
        "    # Compute variance across channel dimension\n",
        "    channel_mean = feature_map.mean(dim=1, keepdim=True)\n",
        "    channel_variance = ((feature_map - channel_mean) ** 2).mean(dim=1, keepdim=True)\n",
        "    \n",
        "    # Apply softmax for attention weights\n",
        "    channel_variance_flat = channel_variance.view(B, T, H * W)\n",
        "    channel_attention_flat = F.softmax(channel_variance_flat, dim=-1)\n",
        "    channel_attention = channel_attention_flat.view(B, 1, T, H, W)\n",
        "    \n",
        "    # === Temporal Variance Attention ===\n",
        "    # Compute variance across temporal dimension\n",
        "    temporal_mean = feature_map.mean(dim=2, keepdim=True)\n",
        "    temporal_variance = ((feature_map - temporal_mean) ** 2).mean(dim=2, keepdim=True)\n",
        "    \n",
        "    # Apply softmax for attention weights\n",
        "    temporal_variance_flat = temporal_variance.view(B, C, H * W)\n",
        "    temporal_attention_flat = F.softmax(temporal_variance_flat, dim=-1)\n",
        "    temporal_attention = temporal_attention_flat.view(B, C, 1, H, W)\n",
        "    \n",
        "    return channel_attention, temporal_attention\n",
        "\n",
        "print(\"‚úì Variance attention mechanism defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7Ô∏è‚É£ Loss Functions\n",
        "\n",
        "### üìä Two Types of Losses:\n",
        "\n",
        "1. **Appearance Loss** (MSE on pixel values)\n",
        "   - Measures how well we reconstruct the visual content\n",
        "   - Normal scenes ‚Üí low loss\n",
        "   - Anomalies ‚Üí high loss (can't reconstruct well)\n",
        "\n",
        "2. **Motion Loss** (MSE on temporal gradients)\n",
        "   - Measures how well we reconstruct movement patterns\n",
        "   - Normal motion ‚Üí low loss\n",
        "   - Anomalous motion ‚Üí high loss\n",
        "\n",
        "### ‚öñÔ∏è Combined Loss:\n",
        "`Total Loss = Œ±‚ÇÅ √ó Appearance Loss + Œ±‚ÇÇ √ó Motion Loss`\n",
        "\n",
        "where Œ±‚ÇÅ = 1.0 and Œ±‚ÇÇ = 25.0 (motion is more important for anomaly detection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def appearance_loss(reconstruction, target, attention_weights=None):\n",
        "    \"\"\"\n",
        "    Compute appearance loss with optional attention weighting.\n",
        "    \n",
        "    Args:\n",
        "        reconstruction: Reconstructed features\n",
        "        target: Original features\n",
        "        attention_weights: Optional variance attention weights\n",
        "    \n",
        "    Returns:\n",
        "        MSE loss (weighted if attention provided)\n",
        "    \"\"\"\n",
        "    loss = F.mse_loss(reconstruction, target, reduction='none')\n",
        "    \n",
        "    if attention_weights is not None:\n",
        "        loss = loss * attention_weights  # Weight by attention\n",
        "    \n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def motion_loss(reconstruction, target, attention_weights=None):\n",
        "    \"\"\"\n",
        "    Compute motion loss using temporal gradients.\n",
        "    \n",
        "    Args:\n",
        "        reconstruction: Reconstructed features (B, C, T, H, W)\n",
        "        target: Original features (B, C, T, H, W)\n",
        "        attention_weights: Optional variance attention weights\n",
        "    \n",
        "    Returns:\n",
        "        MSE loss on temporal gradients\n",
        "    \"\"\"\n",
        "    # Compute temporal gradients (difference between consecutive frames)\n",
        "    target_gradient = torch.abs(target[:, :, 1:] - target[:, :, :-1])\n",
        "    recon_gradient = torch.abs(reconstruction[:, :, 1:] - reconstruction[:, :, :-1])\n",
        "    \n",
        "    loss = F.mse_loss(recon_gradient, target_gradient, reduction='none')\n",
        "    \n",
        "    if attention_weights is not None:\n",
        "        # Adjust attention for temporal dimension (T-1 frames)\n",
        "        attention_weights = attention_weights[:, :, 1:]\n",
        "        loss = loss * attention_weights\n",
        "    \n",
        "    return loss.mean()\n",
        "\n",
        "print(\"‚úì Loss functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8Ô∏è‚É£ Training Module (PyTorch Lightning)\n",
        "\n",
        "### ‚ö° Why PyTorch Lightning?\n",
        "- Cleaner code organization\n",
        "- Automatic GPU handling\n",
        "- Built-in logging and checkpointing\n",
        "- Easy to use with Kaggle\n",
        "\n",
        "### üîÑ Training Process:\n",
        "1. Forward pass through encoder-decoder\n",
        "2. Compute multi-layer reconstruction losses\n",
        "3. Apply variance attention\n",
        "4. Backpropagate and update weights\n",
        "5. Log metrics for monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AnomalyDetectionModel(pl.LightningModule):\n",
        "    \"\"\"PyTorch Lightning module for video anomaly detection\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.config = config\n",
        "        \n",
        "        # Build model\n",
        "        self.model = MultiLayerAutoencoder(in_channels=config.CHANNELS)\n",
        "        \n",
        "        # For validation metrics\n",
        "        self.validation_step_outputs = []\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def compute_multi_layer_loss(self, features, target, mode='train'):\n",
        "        \"\"\"\n",
        "        Compute multi-layer reconstruction loss.\n",
        "        \n",
        "        We reconstruct at multiple layers (input, layer 3, layer 4)\n",
        "        to capture anomalies at different abstraction levels.\n",
        "        \"\"\"\n",
        "        total_app_loss = 0.0\n",
        "        total_mot_loss = 0.0\n",
        "        \n",
        "        # Reverse decoder features to match encoder indexing\n",
        "        dec_to_enc_map = {0: 0, 1: 3, 2: 2, 3: 1, 4: 1}\n",
        "        \n",
        "        for layer_idx in self.config.RECON_LAYERS:\n",
        "            enc_feat = features['enc'][layer_idx]\n",
        "            dec_feat = features['dec'][dec_to_enc_map[layer_idx]]\n",
        "            \n",
        "            # Resize decoder features if needed\n",
        "            if enc_feat.shape != dec_feat.shape:\n",
        "                dec_feat = F.interpolate(dec_feat, size=enc_feat.shape[2:], \n",
        "                                        mode='trilinear', align_corners=True)\n",
        "            \n",
        "            # Compute variance attention\n",
        "            channel_att, temporal_att = compute_variance_attention(enc_feat)\n",
        "            attention = channel_att + temporal_att\n",
        "            \n",
        "            # Appearance loss\n",
        "            app_loss = appearance_loss(dec_feat, enc_feat, attention)\n",
        "            total_app_loss += app_loss\n",
        "            \n",
        "            # Motion loss\n",
        "            mot_loss = motion_loss(dec_feat, enc_feat, attention)\n",
        "            total_mot_loss += mot_loss\n",
        "        \n",
        "        # Average over layers\n",
        "        total_app_loss /= len(self.config.RECON_LAYERS)\n",
        "        total_mot_loss /= len(self.config.RECON_LAYERS)\n",
        "        \n",
        "        # Combined loss\n",
        "        total_loss = self.config.ALPHA_1 * total_app_loss + self.config.ALPHA_2 * total_mot_loss\n",
        "        \n",
        "        return total_loss, total_app_loss, total_mot_loss\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        frames = batch['frames']\n",
        "        \n",
        "        # Forward pass\n",
        "        reconstruction, features = self.model(frames)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss, app_loss, mot_loss = self.compute_multi_layer_loss(features, frames, mode='train')\n",
        "        \n",
        "        # Logging\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_app_loss', app_loss)\n",
        "        self.log('train_mot_loss', mot_loss)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        frames = batch['frames']\n",
        "        \n",
        "        # Forward pass\n",
        "        reconstruction, features = self.model(frames)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss, app_loss, mot_loss = self.compute_multi_layer_loss(features, frames, mode='val')\n",
        "        \n",
        "        # Compute anomaly scores for validation\n",
        "        dec_to_enc_map = {0: 0, 1: 3, 2: 2, 3: 1, 4: 1}\n",
        "        app_scores = []\n",
        "        mot_scores = []\n",
        "        \n",
        "        for layer_idx in self.config.RECON_LAYERS:\n",
        "            enc_feat = features['enc'][layer_idx]\n",
        "            dec_feat = features['dec'][dec_to_enc_map[layer_idx]]\n",
        "            \n",
        "            if enc_feat.shape != dec_feat.shape:\n",
        "                dec_feat = F.interpolate(dec_feat, size=enc_feat.shape[2:], \n",
        "                                        mode='trilinear', align_corners=True)\n",
        "            \n",
        "            # Frame-level scores\n",
        "            app_score = F.mse_loss(dec_feat, enc_feat, reduction='none').mean(dim=[1, 3, 4])\n",
        "            app_scores.append(app_score)\n",
        "            \n",
        "            # Motion scores\n",
        "            enc_grad = torch.abs(enc_feat[:, :, 1:] - enc_feat[:, :, :-1])\n",
        "            dec_grad = torch.abs(dec_feat[:, :, 1:] - dec_feat[:, :, :-1])\n",
        "            mot_score = F.mse_loss(dec_grad, enc_grad, reduction='none').mean(dim=[1, 3, 4])\n",
        "            mot_scores.append(mot_score)\n",
        "        \n",
        "        # Store for epoch end\n",
        "        self.validation_step_outputs.append({\n",
        "            'loss': loss,\n",
        "            'app_loss': app_loss,\n",
        "            'mot_loss': mot_loss\n",
        "        })\n",
        "        \n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def on_validation_epoch_end(self):\n",
        "        avg_loss = torch.stack([x['loss'] for x in self.validation_step_outputs]).mean()\n",
        "        self.log('val_loss_epoch', avg_loss, prog_bar=True)\n",
        "        self.validation_step_outputs.clear()\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        # AdamW optimizer with weight decay\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.config.LEARNING_RATE,\n",
        "            weight_decay=self.config.WEIGHT_DECAY\n",
        "        )\n",
        "        \n",
        "        # Learning rate scheduler (reduce LR every 80 epochs)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "            optimizer,\n",
        "            step_size=80,\n",
        "            gamma=0.5\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': scheduler,\n",
        "                'interval': 'epoch'\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(\"‚úì PyTorch Lightning module defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9Ô∏è‚É£ Testing & CSV Generation\n",
        "\n",
        "### üìù Output Format\n",
        "The function generates a CSV file in Kaggle submission format:\n",
        "```\n",
        "Id,Predicted\n",
        "1_1,0.234567\n",
        "1_2,0.456789\n",
        "...\n",
        "```\n",
        "\n",
        "### üîç Process:\n",
        "1. Load each test video\n",
        "2. Process frames in sliding windows (stride=1)\n",
        "3. Compute appearance & motion scores\n",
        "4. Average overlapping windows\n",
        "5. Normalize scores to [0, 1]\n",
        "6. Save to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_and_generate_csv_by_video(model, test_dataset, config):\n",
        "    \"\"\"\n",
        "    Process each video separately to ensure correct frame numbering.\n",
        "    Output format: Id,Predicted (e.g., 1_1, 1_2, ...)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Get unique videos\n",
        "    video_folders = sorted([d for d in os.listdir(config.TEST_PATH) \n",
        "                           if os.path.isdir(os.path.join(config.TEST_PATH, d))])\n",
        "    \n",
        "    all_results = []\n",
        "    \n",
        "    print(\"Processing videos...\")\n",
        "    for video_idx, video_folder in enumerate(video_folders, start=1):\n",
        "        video_path = os.path.join(config.TEST_PATH, video_folder)\n",
        "        frames_paths = sorted(glob.glob(os.path.join(video_path, '*.jpg')))\n",
        "        \n",
        "        print(f\"\\nVideo {video_idx}/{len(video_folders)}: {video_folder} ({len(frames_paths)} frames)\")\n",
        "        \n",
        "        video_scores = []\n",
        "        \n",
        "        # Process video in sliding windows\n",
        "        for start_idx in range(0, len(frames_paths) - config.SEQUENCE_LENGTH + 1):\n",
        "            sequence_paths = frames_paths[start_idx:start_idx + config.SEQUENCE_LENGTH]\n",
        "            \n",
        "            # Load frames\n",
        "            frames_list = []\n",
        "            for frame_path in sequence_paths:\n",
        "                img = Image.open(frame_path).convert('RGB')\n",
        "                img = img.resize((config.IMG_WIDTH, config.IMG_HEIGHT))\n",
        "                img = np.array(img) / 255.0\n",
        "                frames_list.append(img)\n",
        "            \n",
        "            frames_tensor = np.stack(frames_list, axis=0)\n",
        "            frames_tensor = torch.from_numpy(frames_tensor).float().permute(3, 0, 1, 2)\n",
        "            frames_tensor = frames_tensor.unsqueeze(0).to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                reconstruction, features = model.model(frames_tensor)\n",
        "                \n",
        "                dec_to_enc_map = {0: 0, 1: 3, 2: 2, 3: 1, 4: 1}\n",
        "                app_scores_all = []\n",
        "                mot_scores_all = []\n",
        "                \n",
        "                for layer_idx in config.RECON_LAYERS:\n",
        "                    enc_feat = features['enc'][layer_idx]\n",
        "                    dec_feat = features['dec'][dec_to_enc_map[layer_idx]]\n",
        "                    \n",
        "                    if enc_feat.shape != dec_feat.shape:\n",
        "                        dec_feat = F.interpolate(dec_feat, size=enc_feat.shape[2:], \n",
        "                                                mode='trilinear', align_corners=True)\n",
        "                    \n",
        "                    # Appearance scores\n",
        "                    app_score = F.mse_loss(dec_feat, enc_feat, reduction='none').mean(dim=[1, 3, 4])\n",
        "                    app_scores_all.append(app_score)\n",
        "                    \n",
        "                    # Motion scores\n",
        "                    enc_grad = torch.abs(enc_feat[:, :, 1:] - enc_feat[:, :, :-1])\n",
        "                    dec_grad = torch.abs(dec_feat[:, :, 1:] - dec_feat[:, :, :-1])\n",
        "                    mot_score = F.mse_loss(dec_grad, enc_grad, reduction='none').mean(dim=[1, 3, 4])\n",
        "                    mot_scores_all.append(mot_score)\n",
        "                \n",
        "                # Average across layers\n",
        "                app_score = torch.stack(app_scores_all).mean(dim=0).squeeze(0)\n",
        "                mot_score = torch.stack(mot_scores_all).mean(dim=0).squeeze(0)\n",
        "                mot_score_padded = F.pad(mot_score, (0, 1), value=mot_score[-1])\n",
        "                \n",
        "                # Combine appearance and motion scores\n",
        "                for t in range(config.SEQUENCE_LENGTH):\n",
        "                    frame_score = (config.BETA_1 * app_score[t].item() + \n",
        "                                  config.BETA_2 * mot_score_padded[t].item())\n",
        "                    \n",
        "                    frame_global_idx = start_idx + t\n",
        "                    while len(video_scores) <= frame_global_idx:\n",
        "                        video_scores.append([])\n",
        "                    video_scores[frame_global_idx].append(frame_score)\n",
        "        \n",
        "        # Average overlapping window scores\n",
        "        for frame_num, scores in enumerate(video_scores, start=1):\n",
        "            if scores:\n",
        "                all_results.append({\n",
        "                    'video_id': video_idx,\n",
        "                    'frame_num': frame_num,\n",
        "                    'score': np.mean(scores)\n",
        "                })\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_results)\n",
        "    \n",
        "    # Normalize scores to [0, 1]\n",
        "    min_score = df['score'].min()\n",
        "    max_score = df['score'].max()\n",
        "    df['score_normalized'] = (df['score'] - min_score) / (max_score - min_score)\n",
        "    \n",
        "    # Create competition format\n",
        "    df['Id'] = df['video_id'].astype(str) + '_' + df['frame_num'].astype(str)\n",
        "    df['Predicted'] = df['score_normalized'].round(6)\n",
        "    \n",
        "    # Select only required columns and sort\n",
        "    output_df = df[['Id', 'Predicted']].sort_values('Id')\n",
        "    \n",
        "    # Save to CSV\n",
        "    output_df.to_csv(config.OUTPUT_CSV, index=False)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"‚úì Results saved to {config.OUTPUT_CSV}\")\n",
        "    print(f\"‚úì Total predictions: {len(output_df)}\")\n",
        "    print(f\"‚úì Videos: {df['video_id'].nunique()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    return output_df\n",
        "\n",
        "print(\"‚úì Testing function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üöÄ Main Training & Evaluation\n",
        "\n",
        "Now we're ready to run the complete pipeline:\n",
        "\n",
        "1. **Validate dataset** (count frames)\n",
        "2. **Load datasets** (train sequences)\n",
        "3. **Create data loaders** (batching)\n",
        "4. **Build model** (autoencoder)\n",
        "5. **Train** (with early stopping)\n",
        "6. **Test** (generate anomaly scores)\n",
        "7. **Save CSV** (Kaggle submission format)\n",
        "\n",
        "### ‚è±Ô∏è Expected Training Time (P100 GPU):\n",
        "- Per epoch: ~4-5 minutes\n",
        "- With early stopping (8 epochs patience): **~2-4 hours**\n",
        "- Maximum (80 epochs): ~6-7 hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
        "    \n",
        "    # Set random seed for reproducibility\n",
        "    pl.seed_everything(42)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"üé¨ STARTING VIDEO ANOMALY DETECTION PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # ========== STEP 1: VALIDATE DATASET ==========\n",
        "    frame_counts = get_dataset_frame_counts(config.TRAIN_PATH, config.TEST_PATH)\n",
        "    \n",
        "    # ========== STEP 2: CREATE DATASETS ==========\n",
        "    print(\"\\nLoading datasets...\")\n",
        "    train_dataset = AvenueDataset(\n",
        "        root_dir=config.TRAIN_PATH,\n",
        "        sequence_length=config.SEQUENCE_LENGTH,\n",
        "        mode='train'\n",
        "    )\n",
        "\n",
        "    # Split 20% for validation\n",
        "    val_size = int(0.2 * len(train_dataset))\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_subset, val_subset = torch.utils.data.random_split(\n",
        "        train_dataset, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    print(f\"‚úì Train sequences: {train_size}\")\n",
        "    print(f\"‚úì Validation sequences: {val_size}\")\n",
        "    \n",
        "    # ========== STEP 3: CREATE DATA LOADERS ==========\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úì Train sequences: {train_size}\")\n",
        "    print(f\"‚úì Validation sequences: {val_size}\")\n",
        "    \n",
        "    # üîç ADD THIS DIAGNOSTIC CODE HERE ‚Üì\n",
        "    print(\"\\nüîç DIAGNOSTIC:\")\n",
        "    print(f\"  Validation batches per epoch: {val_size // config.BATCH_SIZE}\")\n",
        "    if val_size < 50:\n",
        "        print(\"  ‚ö†Ô∏è  WARNING: Validation set is very small!\")\n",
        "        print(\"  ‚ö†Ô∏è  This can cause val_loss to be unstable/incorrect\")\n",
        "    # üîç END OF DIAGNOSTIC CODE ‚Üë\n",
        "\n",
        "    # ========== STEP 4: BUILD MODEL ==========\n",
        "    print(\"\\nBuilding model...\")\n",
        "    model = AnomalyDetectionModel(config)\n",
        "    \n",
        "    # ========== STEP 5: SETUP CALLBACKS ==========\n",
        "    # Early stopping: stops if val_loss doesn't improve for 8 epochs\n",
        "    early_stop_callback = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=config.PATIENCE,\n",
        "        mode='min',\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    # Model checkpoint: saves best model\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=config.CHECKPOINT_DIR,\n",
        "        filename='anomaly-{epoch:02d}-{val_loss:.4f}',\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        save_top_k=3\n",
        "    )\n",
        "    \n",
        "    # ========== STEP 6: CREATE TRAINER ==========\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=config.MAX_EPOCHS,\n",
        "        callbacks=[early_stop_callback, checkpoint_callback],\n",
        "        accelerator='auto',  # Automatically use GPU if available\n",
        "        devices=1,\n",
        "        precision=32,  # Mixed precision for faster training\n",
        "        log_every_n_steps=10,\n",
        "        accumulate_grad_batches=4,\n",
        "        gradient_clip_val=1.0\n",
        "    )\n",
        "    \n",
        "    # ========== STEP 7: TRAIN MODEL ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üèãÔ∏è  STARTING TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    \n",
        "    # ========== STEP 8: TEST AND GENERATE CSV ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üß™ TESTING ON TEST DATASET\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    best_model_path = checkpoint_callback.best_model_path\n",
        "    print(f\"Loading best model from: {best_model_path}\")\n",
        "    \n",
        "    best_model = AnomalyDetectionModel.load_from_checkpoint(best_model_path, config=config)\n",
        "    \n",
        "    test_dataset = AvenueDataset(\n",
        "        root_dir=config.TEST_PATH,\n",
        "        sequence_length=config.SEQUENCE_LENGTH,\n",
        "        mode='test'\n",
        "    )\n",
        "    \n",
        "    # Generate predictions\n",
        "    results_df = test_and_generate_csv_by_video(best_model, test_dataset, config)\n",
        "    \n",
        "    # ========== STEP 9: DISPLAY RESULTS ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ TRAINING AND TESTING COMPLETED!\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(\"\\nüìä Sample output (first 20 rows):\")\n",
        "    print(results_df.head(20).to_string(index=False))\n",
        "    \n",
        "    print(\"\\nüìä Sample output (last 20 rows):\")\n",
        "    print(results_df.tail(20).to_string(index=False))\n",
        "    \n",
        "    print(\"\\nüìà Anomaly score statistics:\")\n",
        "    print(results_df['Predicted'].describe())\n",
        "    \n",
        "    print(f\"\\nüéâ Done! Submit '{config.OUTPUT_CSV}' to Kaggle!\")\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "print(\"‚úì Main function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéØ Run Training & Evaluation\n",
        "\n",
        "Execute the cell below to start the complete pipeline. This will:\n",
        "- Validate your dataset\n",
        "- Train the model (with progress bars)\n",
        "- Generate anomaly scores\n",
        "- Save results to `anomaly_scores.csv`\n",
        "\n",
        "**Note**: This may take 2-6 hours depending on your GPU and early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the complete pipeline\n",
        "if __name__ == '__main__':\n",
        "    results = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéä Next Steps\n",
        "\n",
        "### üì§ Submit to Kaggle\n",
        "1. Download `anomaly_scores.csv`\n",
        "2. Go to the competition page\n",
        "3. Submit your results!\n",
        "\n",
        "### üîß Hyperparameter Tuning (Optional)\n",
        "\n",
        "If you want to improve results, try adjusting:\n",
        "\n",
        "```python\n",
        "# In Config class:\n",
        "BATCH_SIZE = 8           # Smaller batch = more updates\n",
        "LEARNING_RATE = 0.0002   # Higher LR = faster learning\n",
        "ALPHA_2 = 30.0           # More weight on motion\n",
        "BETA_2 = 3.0             # More weight on motion scores\n",
        "```\n",
        "\n",
        "### üìä Visualize Results (Optional)\n",
        "\n",
        "Add this cell to visualize anomaly score distribution:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(results['Predicted'].values)\n",
        "plt.xlabel('Frame Index')\n",
        "plt.ylabel('Anomaly Score')\n",
        "plt.title('Anomaly Scores Across All Test Frames')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### üß† Model Insights\n",
        "\n",
        "- **High scores** (close to 1.0) = Anomalous frames\n",
        "- **Low scores** (close to 0.0) = Normal frames\n",
        "- The model learns what \"normal\" looks like during training\n",
        "- Anything that deviates significantly gets a high anomaly score\n",
        "\n",
        "---\n",
        "\n",
        "**Good luck with your submission! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
